{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Experiments with Existing Algorithms: Part 1, Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary experiment consists of three algorithms: learning fair representations (pre-processing), adversarial debiasing (in-processing), and calibrayed equalized odds (post-processing). In order to compare these algorithms, Adult census and German credit datasets will be used. Logistic regression classifier will be used to train and test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")  \n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "\n",
    "#datasets\n",
    "from aif360.datasets import AdultDataset, GermanDataset\n",
    "#functions to pre-process datasets \n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_german\n",
    "\n",
    "#metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "#algorithm\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "\n",
    "#scalers & classifiers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#markdown and plotting\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the datassets\n",
    "\n",
    "#Adult\n",
    "adult_orig = load_preproc_data_adult()\n",
    "adult_orig_train, adult_orig_test = adult_orig.split([0.7], shuffle=True)\n",
    "\n",
    "a_privileged_groups = [{'sex': 1.0}]\n",
    "a_unprivileged_groups = [{'sex': 0.0}]\n",
    "\n",
    "#German credit\n",
    "german_orig = GermanDataset(\n",
    "    protected_attribute_names=['age'],          \n",
    "    privileged_classes=[lambda x: x >= 25],\n",
    "    # ignore sex-related attributes in order to focus on single binary sensitive attribute\n",
    "    features_to_drop=['personal_status', 'sex']\n",
    ")\n",
    "\n",
    "german_orig.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    if (german_orig.labels[i] == 2.0):\n",
    "        german_orig.labels[i] = 0\n",
    "    else:\n",
    "        german_orig.labels[i] = 1\n",
    "        \n",
    "german_orig.favorable_label = 1\n",
    "german_orig.unfavorable_label = 0\n",
    "\n",
    "g_privileged_groups = [{'age': 1}]\n",
    "g_unprivileged_groups = [{'age': 0}]\n",
    "\n",
    "german_orig_train, german_orig_test = german_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_orig.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "#It finda that German dataset has a class imbalance, 700 positive and 300 negative outcomes.\n",
    "k=0\n",
    "for i in range(1000):\n",
    "    if(german_orig.labels[i] == 1):\n",
    "        k+=1\n",
    "    else:\n",
    "        pass\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### German Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 57)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['month', 'credit_amount', 'investment_as_income_percentage', 'residence_since', 'age', 'number_of_credits', 'people_liable_for', 'status=A11', 'status=A12', 'status=A13', 'status=A14', 'credit_history=A30', 'credit_history=A31', 'credit_history=A32', 'credit_history=A33', 'credit_history=A34', 'purpose=A40', 'purpose=A41', 'purpose=A410', 'purpose=A42', 'purpose=A43', 'purpose=A44', 'purpose=A45', 'purpose=A46', 'purpose=A48', 'purpose=A49', 'savings=A61', 'savings=A62', 'savings=A63', 'savings=A64', 'savings=A65', 'employment=A71', 'employment=A72', 'employment=A73', 'employment=A74', 'employment=A75', 'other_debtors=A101', 'other_debtors=A102', 'other_debtors=A103', 'property=A121', 'property=A122', 'property=A123', 'property=A124', 'installment_plans=A141', 'installment_plans=A142', 'installment_plans=A143', 'housing=A151', 'housing=A152', 'housing=A153', 'skill_level=A171', 'skill_level=A172', 'skill_level=A173', 'skill_level=A174', 'telephone=A191', 'telephone=A192', 'foreign_worker=A201', 'foreign_worker=A202']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Adult Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34189, 18)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
     ]
    }
   ],
   "source": [
    "# some information of each dataset regarding labels, names, etc. \n",
    "display(Markdown(\"#### German Training Dataset shape\"))\n",
    "print(german_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(german_orig_train.favorable_label, german_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(german_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(german_orig_train.privileged_protected_attributes, \n",
    "      german_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(german_orig_train.feature_names)\n",
    "\n",
    "display(Markdown(\"#### Adult Training Dataset shape\"))\n",
    "print(adult_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(adult_orig_train.favorable_label, adult_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(adult_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(adult_orig_train.privileged_protected_attributes, \n",
    "      adult_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(adult_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features                \\\n",
       "                                                         \n",
       "                                   month credit_amount   \n",
       "instance names                                           \n",
       "784                         1.0     20.0        6468.0   \n",
       "986                         1.0     42.0        6289.0   \n",
       "534                         1.0     24.0        3105.0   \n",
       "706                         1.0     48.0        6560.0   \n",
       "265                         1.0     15.0         802.0   \n",
       "...                         ...      ...           ...   \n",
       "462                         1.0     12.0        3017.0   \n",
       "437                         1.0     24.0        1287.0   \n",
       "990                         1.0     12.0        3565.0   \n",
       "939                         1.0     24.0        6842.0   \n",
       "221                         1.0     12.0        1200.0   \n",
       "\n",
       "                                                                \\\n",
       "                                                                 \n",
       "               investment_as_income_percentage residence_since   \n",
       "instance names                                                   \n",
       "784                                        1.0             4.0   \n",
       "986                                        2.0             1.0   \n",
       "534                                        4.0             2.0   \n",
       "706                                        3.0             2.0   \n",
       "265                                        4.0             3.0   \n",
       "...                                        ...             ...   \n",
       "462                                        3.0             1.0   \n",
       "437                                        4.0             4.0   \n",
       "990                                        2.0             1.0   \n",
       "939                                        2.0             4.0   \n",
       "221                                        4.0             4.0   \n",
       "\n",
       "                                                                        \\\n",
       "               protected attribute                                       \n",
       "                               age number_of_credits people_liable_for   \n",
       "instance names                                                           \n",
       "784                            1.0               1.0               1.0   \n",
       "986                            1.0               2.0               1.0   \n",
       "534                            1.0               2.0               1.0   \n",
       "706                            0.0               1.0               1.0   \n",
       "265                            1.0               1.0               2.0   \n",
       "...                            ...               ...               ...   \n",
       "462                            1.0               1.0               1.0   \n",
       "437                            1.0               2.0               1.0   \n",
       "990                            1.0               2.0               2.0   \n",
       "939                            1.0               2.0               2.0   \n",
       "221                            0.0               1.0               1.0   \n",
       "\n",
       "                                      ...                                \\\n",
       "                                      ...                                 \n",
       "               status=A11 status=A12  ... housing=A153 skill_level=A171   \n",
       "instance names                        ...                                 \n",
       "784                   0.0        1.0  ...          0.0              0.0   \n",
       "986                   0.0        0.0  ...          0.0              0.0   \n",
       "534                   0.0        0.0  ...          0.0              0.0   \n",
       "706                   0.0        1.0  ...          0.0              0.0   \n",
       "265                   0.0        1.0  ...          0.0              0.0   \n",
       "...                   ...        ...  ...          ...              ...   \n",
       "462                   0.0        1.0  ...          0.0              0.0   \n",
       "437                   0.0        0.0  ...          0.0              0.0   \n",
       "990                   0.0        0.0  ...          0.0              0.0   \n",
       "939                   0.0        0.0  ...          0.0              0.0   \n",
       "221                   1.0        0.0  ...          0.0              0.0   \n",
       "\n",
       "                                                                   \\\n",
       "                                                                    \n",
       "               skill_level=A172 skill_level=A173 skill_level=A174   \n",
       "instance names                                                      \n",
       "784                         0.0              0.0              1.0   \n",
       "986                         0.0              1.0              0.0   \n",
       "534                         0.0              1.0              0.0   \n",
       "706                         0.0              1.0              0.0   \n",
       "265                         0.0              1.0              0.0   \n",
       "...                         ...              ...              ...   \n",
       "462                         0.0              0.0              1.0   \n",
       "437                         0.0              1.0              0.0   \n",
       "990                         1.0              0.0              0.0   \n",
       "939                         0.0              0.0              1.0   \n",
       "221                         0.0              1.0              0.0   \n",
       "\n",
       "                                                                  \\\n",
       "                                                                   \n",
       "               telephone=A191 telephone=A192 foreign_worker=A201   \n",
       "instance names                                                     \n",
       "784                       0.0            1.0                 1.0   \n",
       "986                       1.0            0.0                 1.0   \n",
       "534                       1.0            0.0                 1.0   \n",
       "706                       1.0            0.0                 1.0   \n",
       "265                       1.0            0.0                 1.0   \n",
       "...                       ...            ...                 ...   \n",
       "462                       1.0            0.0                 1.0   \n",
       "437                       0.0            1.0                 1.0   \n",
       "990                       1.0            0.0                 1.0   \n",
       "939                       0.0            1.0                 1.0   \n",
       "221                       0.0            1.0                 1.0   \n",
       "\n",
       "                                   labels  \n",
       "                                           \n",
       "               foreign_worker=A202         \n",
       "instance names                             \n",
       "784                            0.0    1.0  \n",
       "986                            0.0    1.0  \n",
       "534                            0.0    1.0  \n",
       "706                            0.0    0.0  \n",
       "265                            0.0    0.0  \n",
       "...                            ...    ...  \n",
       "462                            0.0    1.0  \n",
       "437                            0.0    1.0  \n",
       "990                            0.0    1.0  \n",
       "939                            0.0    1.0  \n",
       "221                            0.0    1.0  \n",
       "\n",
       "[300 rows x 59 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_orig_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.000e+01, 6.468e+03, 1.000e+00, ..., 1.000e+00, 1.000e+00,\n",
       "        0.000e+00],\n",
       "       [4.200e+01, 6.289e+03, 2.000e+00, ..., 0.000e+00, 1.000e+00,\n",
       "        0.000e+00],\n",
       "       [2.400e+01, 3.105e+03, 4.000e+00, ..., 0.000e+00, 1.000e+00,\n",
       "        0.000e+00],\n",
       "       ...,\n",
       "       [1.200e+01, 3.565e+03, 2.000e+00, ..., 0.000e+00, 1.000e+00,\n",
       "        0.000e+00],\n",
       "       [2.400e+01, 6.842e+03, 2.000e+00, ..., 1.000e+00, 1.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.200e+01, 1.200e+03, 4.000e+00, ..., 1.000e+00, 1.000e+00,\n",
       "        0.000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_orig_test.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Adult original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact (of original labels) between unprivileged and privileged groups = 0.360776\n",
      "Difference in statistical parity (of original labels) between unprivileged and privileged groups = -0.194909\n",
      "Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = 0.725216\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Adult original test dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact (of original labels) between unprivileged and privileged groups = 0.356944\n",
      "Difference in statistical parity (of original labels) between unprivileged and privileged groups = -0.193624\n",
      "Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = 0.723729\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### German original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of original labels) between unprivileged and privileged groups = 0.843577\n",
      "Difference in statistical parity (of original labels) between unprivileged and privileged groups = -0.112943\n",
      "Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = 0.681429\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### German original test dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of original labels) between unprivileged and privileged groups = 0.755583\n",
      "Difference in statistical parity (of original labels) between unprivileged and privileged groups = -0.174182\n",
      "Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = 0.670000\n"
     ]
    }
   ],
   "source": [
    "# Initial disparities in the original datasets\n",
    "\n",
    "#Adult\n",
    "metric_ad_orig_train = BinaryLabelDatasetMetric(adult_orig_train, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "display(Markdown(\"#### Adult original training dataset\"))\n",
    "\n",
    "print(\"Disparate impact (of original labels) between unprivileged and privileged groups = %f\" % metric_ad_orig_train.disparate_impact())\n",
    "print(\"Difference in statistical parity (of original labels) between unprivileged and privileged groups = %f\" % metric_ad_orig_train.statistical_parity_difference())\n",
    "print(\"Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = %f\" % metric_ad_orig_train.consistency())\n",
    "\n",
    "metric_ad_orig_test = BinaryLabelDatasetMetric(adult_orig_test, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "display(Markdown(\"#### Adult original test dataset\"))\n",
    "\n",
    "print(\"Disparate impact (of original labels) between unprivileged and privileged groups = %f\" % metric_ad_orig_test.disparate_impact())\n",
    "print(\"Difference in statistical parity (of original labels) between unprivileged and privileged groups = %f\" % metric_ad_orig_test.statistical_parity_difference())\n",
    "print(\"Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = %f\" % metric_ad_orig_test.consistency())\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "#German\n",
    "metric_ger_orig_train = BinaryLabelDatasetMetric(german_orig_train, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "display(Markdown(\"#### German original training dataset\"))\n",
    "\n",
    "print(\"Disparate impact ratio (of original labels) between unprivileged and privileged groups = %f\" % metric_ger_orig_train.disparate_impact())\n",
    "print(\"Difference in statistical parity (of original labels) between unprivileged and privileged groups = %f\" % metric_ger_orig_train.statistical_parity_difference())\n",
    "print(\"Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = %f\" % metric_ger_orig_train.consistency())\n",
    "\n",
    "metric_ger_orig_test = BinaryLabelDatasetMetric(german_orig_test, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "display(Markdown(\"#### German original test dataset\"))\n",
    "\n",
    "print(\"Disparate impact ratio (of original labels) between unprivileged and privileged groups = %f\" % metric_ger_orig_test.disparate_impact())\n",
    "print(\"Difference in statistical parity (of original labels) between unprivileged and privileged groups = %f\" % metric_ger_orig_test.statistical_parity_difference())\n",
    "print(\"Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = %f\" % metric_ger_orig_test.consistency())\n",
    "\n",
    "#n_neighbors warning exists due to how this individual metric is implemented in the library. The function has its own parameter n_neighbors=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial fairness performance of the predictions of a classifier without mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the dataset\n",
    "scale_orig = StandardScaler()\n",
    "\n",
    "#German\n",
    "X_train_g = scale_orig.fit_transform(german_orig_train.features)\n",
    "X_test_g = scale_orig.transform(german_orig_test.features)\n",
    "\n",
    "y_train_g = german_orig_train.labels.ravel()\n",
    "y_test_g = german_orig_test.labels.ravel()\n",
    "\n",
    "#Adult\n",
    "X_train_a = scale_orig.fit_transform(adult_orig_train.features)\n",
    "X_test_a = scale_orig.transform(adult_orig_test.features)\n",
    "\n",
    "y_train_a = adult_orig_train.labels.ravel()\n",
    "y_test_a = adult_orig_test.labels.ravel()\n",
    "\n",
    "\n",
    "#Logistic Regression Training for each dataset\n",
    "log_reg_g = LogisticRegression() \n",
    "log_reg_a = LogisticRegression() \n",
    "\n",
    "#Fitting the German dataset\n",
    "log_reg_g.fit(X_train_g, y_train_g)\n",
    "\n",
    "#Fitting Adult dataset\n",
    "log_reg_a.fit(X_train_a, y_train_a)\n",
    "\n",
    "#Predicting test set labels\n",
    "y_test_pred_g = log_reg_g.predict(X_test_g)\n",
    "y_test_pred_proba_g = log_reg_g.predict_proba(X_test_g)\n",
    "\n",
    "y_test_pred_a = log_reg_a.predict(X_test_a)\n",
    "y_test_pred_proba_a = log_reg_a.predict_proba(X_test_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_pred_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Performance of the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### German Test Set Fairness Performance Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average equalized odds difference between unprivileged and privileged groups = -0.106385\n",
      "Disparate impact ratio between unprivileged and privileged groups = 0.702279\n",
      "Demographic parity difference between unprivileged and privileged groups = -0.184792\n",
      "Predictive Parity difference between unprivileged and privileged groups = -0.251271\n",
      "Consistency of indivuals' predicted labels = 0.635333\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Adult Test Set Fairness Performance Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average equalized odds difference between unprivileged and privileged groups = -0.338793\n",
      "Disparate impact ratio between unprivileged and privileged groups = 0.261540\n",
      "Demographic parity difference between unprivileged and privileged groups = -0.385316\n",
      "Predictive Parity difference between unprivileged and privileged groups = -0.131045\n",
      "Consistency of indivuals' predicted labels = 0.999113\n"
     ]
    }
   ],
   "source": [
    "#1) German Dataset\n",
    "\n",
    "display(Markdown(\"#### German Test Set Fairness Performance Results\"))\n",
    "\n",
    "#Create a new version of the test set with predicted class labels\n",
    "g_testset_pred = german_orig_test.copy()\n",
    "g_testset_pred.labels = y_test_pred_g\n",
    "\n",
    "#Construction 1\n",
    "#to construct this metric function, the predicted labels should be united with the test fetures to make a new datas\n",
    "metric_ger_pred_test = BinaryLabelDatasetMetric(g_testset_pred, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "\n",
    "#Construction 2\n",
    "#both original test dataset with actual labels and the test dataset combined with predicted class labels need to be given to this function\n",
    "classified_metric_g = ClassificationMetric(german_orig_test, \n",
    "                                                 g_testset_pred,\n",
    "                                                 unprivileged_groups=g_unprivileged_groups,\n",
    "                                                 privileged_groups=g_privileged_groups)\n",
    "\n",
    "\n",
    "#Checking Equalized Odds: average odds differecence, which is the avg. of differences in FPR&TPR for privileged and unprivileged groups.\n",
    "aeo_g = classified_metric_g.average_odds_difference()\n",
    "print(\"Average equalized odds difference between unprivileged and privileged groups = %f\" % aeo_g)\n",
    "\n",
    "#Disparate Impact ratio between privileged and unprivileged groups.\n",
    "di_g = classified_metric_g.disparate_impact()\n",
    "print(\"Disparate impact ratio between unprivileged and privileged groups = %f\" % di_g)\n",
    "\n",
    "#Demographic parity difference between privileged and unprivileged groups.\n",
    "spd_g = classified_metric_g.statistical_parity_difference()\n",
    "print(\"Demographic parity difference between unprivileged and privileged groups = %f\" % spd_g)\n",
    "\n",
    "#Predictive parity difference: PPV difference between privileged and unprivileged groups.\n",
    "ppd_g = classified_metric_g.positive_predictive_value(privileged=False) - classified_metric_g.positive_predictive_value(privileged=True)\n",
    "print(\"Predictive Parity difference between unprivileged and privileged groups = %f\" % ppd_g)\n",
    "\n",
    "#Individual Fairness: 1)Consistency, 2) Euclidean Distance between individuals.\n",
    "print(\"Consistency of indivuals' predicted labels = %f\" % metric_ger_pred_test.consistency())\n",
    "\n",
    "\n",
    "\n",
    "#2) Adult Dataset\n",
    "display(Markdown(\"#### Adult Test Set Fairness Performance Results\"))\n",
    "\n",
    "a_testset_pred = adult_orig_test.copy()\n",
    "a_testset_pred.labels = y_test_pred_a\n",
    "\n",
    "#Construction 1\n",
    "#to construct this metric function, the predicted labels should be united with the test fetures to make a new datas\n",
    "metric_ad_pred_test = BinaryLabelDatasetMetric(a_testset_pred, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "\n",
    "\n",
    "#Construction 2\n",
    "#both original test dataset and the test dataset with predicted class labels need to be given to this function\n",
    "classified_metric_a = ClassificationMetric(adult_orig_test, \n",
    "                                                 a_testset_pred,\n",
    "                                                 unprivileged_groups=a_unprivileged_groups,\n",
    "                                                 privileged_groups=a_privileged_groups)\n",
    "\n",
    "#Checking Equalized Odds: average odds differecence, which is the avg. of differences in FPR&TPR for privileged and unprivileged groups.\n",
    "aeo_a = classified_metric_a.average_odds_difference()\n",
    "print(\"Average equalized odds difference between unprivileged and privileged groups = %f\" % aeo_a)\n",
    "\n",
    "#Disparate Impact ratio between privileged and unprivileged groups.\n",
    "di_a = classified_metric_a.disparate_impact()\n",
    "print(\"Disparate impact ratio between unprivileged and privileged groups = %f\" % di_a)\n",
    "\n",
    "#Demographic parity difference between privileged and unprivileged groups.\n",
    "spd_a = classified_metric_a.statistical_parity_difference()\n",
    "print(\"Demographic parity difference between unprivileged and privileged groups = %f\" % spd_a)\n",
    "\n",
    "#Predictive parity difference: PPV difference between privileged and unprivileged groups.\n",
    "ppd_a = classified_metric_a.positive_predictive_value(privileged=False) - classified_metric_a.positive_predictive_value(privileged=True)\n",
    "print(\"Predictive Parity difference between unprivileged and privileged groups = %f\" % ppd_a)\n",
    "\n",
    "#Individual Fairness: 1)Consistency, 2) Euclidean Distance between individuals.\n",
    "print(\"Consistency of indivuals' predicted labels = %f\" % metric_ad_pred_test.consistency())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Performance Metrics Before Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Classifier Prediction Performance on German Test Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy of logistic regression trained on German dataset without any mitigation = 0.686667\n",
      "Balanced accuracy of logistic regression trained on German dataset without any mitigation = 0.675238\n",
      "F1 score of logistic regression trained on German dataset without any mitigation = 0.756477\n",
      "For german dataset\n",
      "Precision (PPV): 0.815642\n",
      "Recall (TPR): 0.705314\n",
      "Specificity (TNR): 0.645161\n",
      "F1-score: 0.756477\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classifier Prediction Performance on Adult Test Set "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy of logistic regression trained on German dataset without any mitigation = 0.733979\n",
      "Balanced accuracy of logistic regression trained on German dataset without any mitigation = 0.746853\n",
      "F1 score of logistic regression trained on German dataset without any mitigation = 0.579141\n",
      "For adult dataset\n",
      "Precision: 0.463613\n",
      "Recall: 0.771355\n",
      "Specificity: 0.722351\n",
      "F1-score: 0.579141\n"
     ]
    }
   ],
   "source": [
    "#German\n",
    "\n",
    "TPRg = classified_metric_g.true_positive_rate() #recall\n",
    "TNRg = classified_metric_g.true_negative_rate() #specificity\n",
    "PPVg = classified_metric_g.positive_predictive_value() #precision\n",
    "bal_acc_g = (TPRg+TNRg)/2\n",
    "f1_g = 2*((PPVg*TPRg)/(PPVg+TPRg))\n",
    "\n",
    "display(Markdown(\"#### Classifier Prediction Performance on German Test Set\"))\n",
    "print(\"Standard accuracy of logistic regression trained on German dataset without any mitigation = %f\" % classified_metric_g.accuracy())\n",
    "print(\"Balanced accuracy of logistic regression trained on German dataset without any mitigation = %f\" % bal_acc_g)\n",
    "print(\"F1 score of logistic regression trained on German dataset without any mitigation = %f\" % f1_g)\n",
    "\n",
    "print(\"For german dataset\")\n",
    "print(\"Precision (PPV): %f\" %PPVg)\n",
    "print(\"Recall (TPR): %f\" %TPRg)\n",
    "print(\"Specificity (TNR): %f\" %TNRg)\n",
    "print(\"F1-score: %f\" %f1_g)\n",
    "\n",
    "#Adult\n",
    "\n",
    "TPRa = classified_metric_a.true_positive_rate()\n",
    "TNRa = classified_metric_a.true_negative_rate()\n",
    "PPVa = classified_metric_a.positive_predictive_value()\n",
    "bal_acc_a = (TPRa+TNRa)/2\n",
    "f1_a = 2*((PPVa*TPRa)/(PPVa+TPRa))\n",
    "\n",
    "display(Markdown(\"#### Classifier Prediction Performance on Adult Test Set \"))\n",
    "print(\"Standard accuracy of logistic regression trained on German dataset without any mitigation = %f\" % classified_metric_a.accuracy())\n",
    "print(\"Balanced accuracy of logistic regression trained on German dataset without any mitigation = %f\" % bal_acc_a)\n",
    "print(\"F1 score of logistic regression trained on German dataset without any mitigation = %f\" % f1_a)\n",
    "\n",
    "print(\"For adult dataset\")\n",
    "print(\"Precision: %f\" %PPVa)\n",
    "print(\"Recall: %f\" %TPRa)\n",
    "print(\"Specificity: %f\" %TNRa)\n",
    "print(\"F1-score: %f\" %f1_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2d4f1de1dd8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm_german = confusion_matrix(german_orig_test.labels, g_testset_pred.labels)\n",
    "\n",
    "disp_german = ConfusionMatrixDisplay(confusion_matrix=cm_german,\n",
    "                              display_labels=log_reg_g.classes_)\n",
    "disp_german.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2d4f2d8b400>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adult = confusion_matrix(adult_orig_test.labels, a_testset_pred.labels)\n",
    "\n",
    "disp_adult = ConfusionMatrixDisplay(confusion_matrix=cm_adult,\n",
    "                              display_labels=log_reg_a.classes_)\n",
    "disp_adult.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the Mitigation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing algorithm: Learning Fair Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 83249.80707600791\n",
      "500 70844.79385786862\n",
      "750 70411.5738023804\n",
      "1000 74264.36657282058\n",
      "1250 70823.80844506534\n",
      "1500 70187.05430589327\n",
      "1750 69284.58697278144\n",
      "2000 68633.965930703\n",
      "2250 65995.8027816055\n",
      "2500 66635.90214005698\n",
      "2750 66424.35773091216\n",
      "3000 66084.30807786873\n",
      "3250 66172.92582256568\n",
      "3500 65234.697085083775\n",
      "3750 64760.020124962524\n",
      "4000 64220.04649473905\n",
      "4250 62563.67119761381\n",
      "4500 61224.4880677521\n",
      "4750 60130.43736764904\n",
      "5000 59893.17098447331\n"
     ]
    }
   ],
   "source": [
    "#1) Transforming Adult Dataset\n",
    "\n",
    "#Required Inputs:\n",
    "# Input recontruction quality - Ax\n",
    "# Fairness constraint - Az\n",
    "# Output prediction error - Ay\n",
    "\n",
    "#scaled dataset together with its labels is needed\n",
    "adult_orig_train.features = scale_orig.fit_transform(adult_orig_train.features)\n",
    "adult_orig_test.features = scale_orig.transform(adult_orig_test.features)\n",
    "\n",
    "#LFR itself contains logistic regression sinc it uses signoid functions \n",
    "LFR_a =LFR(unprivileged_groups=a_unprivileged_groups,\n",
    "         privileged_groups=a_privileged_groups,\n",
    "         k=10, Ax=0.1, Ay=1.0, Az=2.0,\n",
    "         verbose=1\n",
    "        )\n",
    "TR_a = LFR_a.fit(adult_orig_train, maxiter=5000, maxfun=5000)\n",
    "\n",
    "\n",
    "# Transform training data and align features\n",
    "a_transf_train = TR_a.transform(adult_orig_train)\n",
    "a_transf_test = TR_a.transform(adult_orig_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5250 5252.815638453695\n",
      "5500 5252.815909852896\n",
      "5750 5252.815867000124\n",
      "6000 4986.662390841265\n",
      "6250 4986.662397301612\n",
      "6500 4986.66241244289\n",
      "6750 4619.244599182724\n",
      "7000 4619.244915716556\n",
      "7250 4619.244565912454\n",
      "7500 4310.563506027777\n",
      "7750 4310.563593020886\n",
      "8000 4310.563504482912\n",
      "8250 4307.407876128636\n",
      "8500 4307.407888250555\n",
      "8750 4253.468880404726\n",
      "9000 4253.46887062803\n",
      "9250 4253.468870933014\n",
      "9500 4222.046326181713\n",
      "9750 4222.046293215335\n",
      "10000 4222.046333678051\n",
      "10250 4166.814926985774\n",
      "10500 4166.815136461748\n",
      "10750 4166.8150181213\n"
     ]
    }
   ],
   "source": [
    "#2) Transforming German Dataset\n",
    "\n",
    "#scaled dataset together with its labels is needed\n",
    "german_orig_train.features = scale_orig.fit_transform(german_orig_train.features)\n",
    "german_orig_test.features = scale_orig.transform(german_orig_test.features)\n",
    "\n",
    "#LFR itself contains logistic regression sinc it uses signoid functions \n",
    "LFR_g =LFR(unprivileged_groups=g_unprivileged_groups,\n",
    "         privileged_groups=g_privileged_groups,\n",
    "         k=10, Ax=0.1, Ay=1.0, Az=2.0,\n",
    "         verbose=1\n",
    "        )\n",
    "TR_g = LFR_g.fit(german_orig_train, maxiter=5000, maxfun=5000)\n",
    "\n",
    "\n",
    "# Transform training data and align features\n",
    "g_transf_train = TR_g.transform(german_orig_train)\n",
    "g_transf_test = TR_g.transform(german_orig_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation of LFR algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness performance of the datasets before classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed Adult train set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of transformed labels) between unprivileged and privileged groups = 0.531858\n",
      "Difference in statistical parity (of transformed data) between unprivileged and privileged groups = -0.125024\n",
      "Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = 1.000000\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Transformed Adult test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of transformed data) between unprivileged and privileged groups = 0.534977\n",
      "Difference in statistical parity (of transformed data) between unprivileged and privileged groups = -0.118622\n",
      "Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = 1.000000\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Transformed German train set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of transformed data) between unprivileged and privileged groups = 0.725668\n",
      "Difference in statistical parity (of transformed data) between unprivileged and privileged groups = -0.237134\n",
      "Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = 0.990571\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Transformed German test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of transformed data) between unprivileged and privileged groups = 0.760490\n",
      "Difference in statistical parity (of transformed data) between unprivileged and privileged groups = -0.201886\n",
      "Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = 0.982667\n"
     ]
    }
   ],
   "source": [
    "#Fairness Performance of Datasets Before Classification\n",
    "\n",
    "#Constucting two functions to call the desired metrics\n",
    "#Adult\n",
    "metric_transf_train_a = BinaryLabelDatasetMetric(a_transf_train, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed Adult train set\"))\n",
    "print(\"Disparate impact ratio (of transformed labels) between unprivileged and privileged groups = %f\" % metric_transf_train_a.disparate_impact())\n",
    "print(\"Difference in statistical parity (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_train_a.statistical_parity_difference())\n",
    "print(\"Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = %f\" % metric_transf_train_a.consistency())\n",
    "\n",
    "\n",
    "metric_transf_test_a = BinaryLabelDatasetMetric(a_transf_test, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed Adult test set\"))\n",
    "print(\"Disparate impact ratio (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_test_a.disparate_impact())\n",
    "print(\"Difference in statistical parity (of transformed data) between unprivileged and privileged groups = %f\" %metric_transf_test_a.statistical_parity_difference())\n",
    "print(\"Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = %f\" % metric_transf_test_a.consistency())\n",
    "\n",
    "\n",
    "#German\n",
    "metric_transf_train_g = BinaryLabelDatasetMetric(g_transf_train, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed German train set\"))\n",
    "print(\"Disparate impact ratio (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_train_g.disparate_impact())\n",
    "print(\"Difference in statistical parity (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_train_g.statistical_parity_difference())\n",
    "print(\"Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = %f\" % metric_transf_train_g.consistency())\n",
    "\n",
    "\n",
    "metric_transf_test_g = BinaryLabelDatasetMetric(g_transf_test, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed German test set\"))\n",
    "print(\"Disparate impact ratio (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_test_g.disparate_impact())\n",
    "print(\"Difference in statistical parity (of transformed data) between unprivileged and privileged groups = %f\" %metric_transf_test_g.statistical_parity_difference())\n",
    "print(\"Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = %f\" % metric_transf_test_g.consistency())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness and predictive performance after classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " g_transf_test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_orig_test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fairness Performance of Predictions After Classification with Logistic Regression\n",
    "#Note: this time scaling is not applied since the original datasets were already scaled before transformation.\n",
    "#German\n",
    "X_train_g_trans =g_transf_train.features\n",
    "X_test_g_trans = g_transf_test.features\n",
    "\n",
    "y_train_g_trans = g_transf_train.labels.ravel()\n",
    "y_test_g_trans = g_transf_test.labels.ravel()\n",
    "\n",
    "#Adult\n",
    "X_train_a_trans = a_transf_train.features\n",
    "X_test_a_trans = a_transf_test.features\n",
    "\n",
    "y_train_a_trans = a_transf_train.labels.ravel()\n",
    "y_test_a_trans = a_transf_test.labels.ravel()\n",
    "\n",
    "\n",
    "#Logistic Regression Training for each dataset\n",
    "trans_lr_g = LogisticRegression() \n",
    "trans_lr_a = LogisticRegression() \n",
    "\n",
    "#Fitting the German dataset\n",
    "trans_lr_g.fit(X_train_g_trans, y_train_g_trans)\n",
    "\n",
    "#Fitting Adult dataset\n",
    "trans_lr_a.fit(X_train_a_trans, y_train_a_trans)\n",
    "\n",
    "#Predicting test set labels\n",
    "y_test_trans_pred_g = log_reg_g.predict(X_test_g_trans)\n",
    "y_test_trans_pred_proba_g = log_reg_g.predict_proba(X_test_g_trans)\n",
    "\n",
    "y_test_trans_pred_a = log_reg_a.predict(X_test_a_trans)\n",
    "y_test_trans_pred_proba_a = log_reg_a.predict_proba(X_test_a_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### German Transformed Test Set Fairness Performance (based on predictions)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average equalized odds difference between unprivileged and privileged groups = -0.010000\n",
      "Disparate impact ratio between unprivileged and privileged groups = 0.743590\n",
      "Demographic parity difference between unprivileged and privileged groups = -0.194518\n",
      "Predictive Parity difference between unprivileged and privileged groups = 0.000000\n",
      "Consistency of indivuals' predicted labels = 0.968000\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Adult Transformed Test Set Fairness Performance (based on predictions)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average equalized odds difference between unprivileged and privileged groups = -0.152076\n",
      "Disparate impact ratio between unprivileged and privileged groups = 0.357108\n",
      "Demographic parity difference between unprivileged and privileged groups = -0.338133\n",
      "Predictive Parity difference between unprivileged and privileged groups = 0.241570\n",
      "Consistency of indivuals' predicted labels = 0.999959\n"
     ]
    }
   ],
   "source": [
    "#Constructors to retrieve the results\n",
    "\n",
    "#1) German Dataset\n",
    "\n",
    "display(Markdown(\"#### German Transformed Test Set Fairness Performance (based on predictions)\"))\n",
    "\n",
    "#Create a new version of the transformed test set with predicted class labels\n",
    "g_testset_pred_trans = g_transf_test.copy()\n",
    "g_testset_pred_trans.labels = y_test_trans_pred_g\n",
    "\n",
    "#Construction 1\n",
    "#to construct this metric function, the predicted labels should be united with the test fetures to make a new datas\n",
    "metric_ger_pred_trans_test = BinaryLabelDatasetMetric(g_testset_pred_trans, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "\n",
    "#Construction 2\n",
    "'''both transformed test dataset with actual labels and the transformed test dataset combined with predicted class \n",
    "labels need to be given to this function'''\n",
    "classified_metric_trans_g = ClassificationMetric(g_transf_test, \n",
    "                                                 g_testset_pred_trans,\n",
    "                                                 unprivileged_groups=g_unprivileged_groups,\n",
    "                                                 privileged_groups=g_privileged_groups)\n",
    "\n",
    "\n",
    "#Checking Equalized Odds: average odds differecence, which is the avg. of differences in FPR&TPR for privileged and unprivileged groups.\n",
    "t_aeo_g = classified_metric_trans_g.average_odds_difference()\n",
    "print(\"Average equalized odds difference between unprivileged and privileged groups = %f\" % t_aeo_g)\n",
    "\n",
    "#Disparate Impact ratio between privileged and unprivileged groups.\n",
    "t_di_g = classified_metric_trans_g.disparate_impact()\n",
    "print(\"Disparate impact ratio between unprivileged and privileged groups = %f\" % t_di_g)\n",
    "\n",
    "#Demographic parity difference between privileged and unprivileged groups.\n",
    "t_spd_g = classified_metric_trans_g.statistical_parity_difference()\n",
    "print(\"Demographic parity difference between unprivileged and privileged groups = %f\" % t_spd_g)\n",
    "\n",
    "#Predictive parity difference: PPV difference between privileged and unprivileged groups.\n",
    "t_ppd_g = classified_metric_trans_g.positive_predictive_value(privileged=False) - classified_metric_trans_g.positive_predictive_value(privileged=True)\n",
    "print(\"Predictive Parity difference between unprivileged and privileged groups = %f\" % t_ppd_g)\n",
    "\n",
    "#Individual Fairness: 1)Consistency, 2) Euclidean Distance between individuals.\n",
    "print(\"Consistency of indivuals' predicted labels = %f\" % metric_ger_pred_trans_test.consistency())\n",
    "\n",
    "\n",
    "\n",
    "#2) Adult Dataset\n",
    "display(Markdown(\"#### Adult Transformed Test Set Fairness Performance (based on predictions)\"))\n",
    "\n",
    "#Create a new version of the transformed test set with predicted class labels\n",
    "a_testset_pred_trans = a_transf_test.copy()\n",
    "a_testset_pred_trans.labels = y_test_trans_pred_a\n",
    "\n",
    "#Construction 1\n",
    "#to construct this metric function, the predicted labels should be united with the test fetures to make a new datas\n",
    "metric_ad_pred_trans_test = BinaryLabelDatasetMetric(a_testset_pred_trans, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "\n",
    "\n",
    "#Construction 2\n",
    "#both original test dataset and the test dataset with predicted class labels need to be given to this function\n",
    "classified_metric_trans_a = ClassificationMetric(a_transf_test, \n",
    "                                                 a_testset_pred_trans,\n",
    "                                                 unprivileged_groups=a_unprivileged_groups,\n",
    "                                                 privileged_groups=a_privileged_groups)\n",
    "\n",
    "#Checking Equalized Odds: average odds differecence, which is the avg. of differences in FPR&TPR for privileged and unprivileged groups.\n",
    "t_aeo_a = classified_metric_trans_a.average_odds_difference()\n",
    "print(\"Average equalized odds difference between unprivileged and privileged groups = %f\" % t_aeo_a)\n",
    "\n",
    "#Disparate Impact ratio between privileged and unprivileged groups.\n",
    "t_di_a = classified_metric_trans_a.disparate_impact()\n",
    "print(\"Disparate impact ratio between unprivileged and privileged groups = %f\" % t_di_a)\n",
    "\n",
    "#Demographic parity difference between privileged and unprivileged groups.\n",
    "t_spd_a = classified_metric_trans_a.statistical_parity_difference()\n",
    "print(\"Demographic parity difference between unprivileged and privileged groups = %f\" % t_spd_a)\n",
    "\n",
    "#Predictive parity difference: PPV difference between privileged and unprivileged groups.\n",
    "t_ppd_a = classified_metric_trans_a.positive_predictive_value(privileged=False) - classified_metric_trans_a.positive_predictive_value(privileged=True)\n",
    "print(\"Predictive Parity difference between unprivileged and privileged groups = %f\" % t_ppd_a)\n",
    "\n",
    "#Individual Fairness: 1)Consistency, 2) Euclidean Distance between individuals.\n",
    "print(\"Consistency of indivuals' predicted labels = %f\" % metric_ad_pred_trans_test.consistency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Classifier Prediction Performance on Transformed German Test Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy of logistic regression trained on German dataset with LFR mitigation = 0.916667\n",
      "Balanced accuracy of logistic regression trained on German dataset with LFR mitigation = 0.948980\n",
      "F1 score of logistic regression trained on German dataset with LFR mitigation = 0.946237\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classifier Prediction Performance on Transformed Adult Test Set "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy of logistic regression trained on German dataset with LFR mitigation = 0.801474\n",
      "Balanced accuracy of logistic regression trained on German dataset with LFR mitigation = 0.873390\n",
      "F1 score of logistic regression trained on German dataset with LFR mitigation = 0.685139\n"
     ]
    }
   ],
   "source": [
    "#classifier performance\n",
    "\n",
    "#German\n",
    "\n",
    "TPRg_tr = classified_metric_trans_g.true_positive_rate() #recall\n",
    "TNRg_tr = classified_metric_trans_g.true_negative_rate() #specificity\n",
    "PPVg_tr = classified_metric_trans_g.positive_predictive_value() #precision\n",
    "bal_acc_g_tr = (TPRg_tr+TNRg_tr)/2\n",
    "f1_g_tr = 2*((PPVg_tr*TPRg_tr)/(PPVg_tr+TPRg_tr))\n",
    "\n",
    "display(Markdown(\"#### Classifier Prediction Performance on Transformed German Test Set\"))\n",
    "print(\"Standard accuracy of logistic regression trained on German dataset with LFR mitigation = %f\" % classified_metric_trans_g.accuracy())\n",
    "print(\"Balanced accuracy of logistic regression trained on German dataset with LFR mitigation = %f\" % bal_acc_g_tr)\n",
    "print(\"F1 score of logistic regression trained on German dataset with LFR mitigation = %f\" % f1_g_tr)\n",
    "\n",
    "#Adult\n",
    "\n",
    "TPRa_tr = classified_metric_trans_a.true_positive_rate()\n",
    "TNRa_tr = classified_metric_trans_a.true_negative_rate()\n",
    "PPVa_tr = classified_metric_trans_a.positive_predictive_value()\n",
    "bal_acc_a_tr = (TPRa_tr+TNRa_tr)/2\n",
    "f1_a_tr = 2*((PPVa_tr*TPRa_tr)/(PPVa_tr+TPRa_tr))\n",
    "\n",
    "display(Markdown(\"#### Classifier Prediction Performance on Transformed Adult Test Set \"))\n",
    "print(\"Standard accuracy of logistic regression trained on German dataset with LFR mitigation = %f\" % classified_metric_trans_a.accuracy())\n",
    "print(\"Balanced accuracy of logistic regression trained on German dataset with LFR mitigation = %f\" % bal_acc_a_tr)\n",
    "print(\"F1 score of logistic regression trained on German dataset with LFR mitigation = %f\" % f1_a_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2d4f7699be0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Confusion Matrices\n",
    "'''NOTE: THERE MIGHT BE A PROBLEM DUE TO CHANGING CLASS LABELS DURING THE PROCESS'''\n",
    "#Adult\n",
    "cm_a = confusion_matrix(adult_orig_test.labels, a_testset_pred_trans.labels)\n",
    "\n",
    "disp_a = ConfusionMatrixDisplay(confusion_matrix=cm_a,\n",
    "                              display_labels=trans_lr_a.classes_)\n",
    "disp_a.plot() \n",
    "\n",
    "#German\n",
    "cm_g = confusion_matrix(german_orig_test.labels, g_testset_pred_trans.labels)\n",
    "\n",
    "disp_g = ConfusionMatrixDisplay(confusion_matrix=cm_g,\n",
    "                              display_labels=trans_lr_g.classes_)\n",
    "disp_g.plot() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif360",
   "language": "python",
   "name": "aif360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
