\section{Related Work} \label{sec:related}

In this section, we discuss various fairness metrics defined in the literature and the mitigation algorithms proposed. We start by discussing the different fairness metrics.

\subsection{Fairness Metrics}
There are various fairness metrics in the literature that are developed to quantify the fairness/unfairness in a dataset or the outcomes of a system. They can be used to measure the fairness in different stages of the machine learning pipeline. 
% There are two fundamental viewpoints or approaches to the notion of fairness due to its nature, which are axiomatically defined as "What You See Is What You Get (WYSIWYG)" and "We Are All Equal (WAE)"  \cite{friedler2016possibility}. 
% \citeauthor{friedler2016possibility} \cite{friedler2016possibility} formalized fairness as the mapping from "construct space" (the space that captures the whole population's meaningful attributes) to "observed space" (the space that can capture only some observable parts of the construct-space), and then to the "decision space" (the space of outcomes that are predicted). According to WYSIWYG, the collected dataset should reflect the original distribution and the characteristics of the population, while the WAE says that all groups are essentially the same.  The metrics regarding demographic parity-related metrics reflect the WAE view, whereas the equality of odds-related metrics reflect WYSIWYG. The other metrics do not have a certain choice of worldview but they are located in between these two ideas. 

\stitle{Statistical fairness metrics:} the first category of fairness metrics is called \emph{statistical fairness metrics} or \emph{associational fairness} metrics \cite{salimi2019interventional}.  The main idea behind the statistical fairness metrics is that there must be some parity with a small amount of difference in the measurements between the different groups \cite{chouldechova2018frontiers}. These statistical metrics are always applied to groups of people identified in the dataset so they are called group-based metrics. A set of these metrics consider only the predicted outcome while other metrics consider both predicted and actual outcomes, or predicted probabilities and actual outcomes \cite{verma2018fairness_explained}.


An example of statistical fairness metrics is the \emph{demographic parity}, which states that a classification algorithm is fair if the different groups according to a sensitive attribute have the same probability to be assigned to the positive outcome \cite{dwork2012fairness,kamishima2011fairness}. It means that the sensitive attribute and the outcome should be statistically independent of each other. The \emph{conditional statistical parity} considers a small additional set of "legitimate" attributes while checking the parity in the outcome \cite{corbett2017algorithmic}. In other words, the sensitive attribute(s) should be independent of the outcome given (a set of) legitimate attributes. 

A metric emanated from a legal rule \cite{US_guideline} and formulated by \citeauthor{feldman2015certifying} is called \emph{disparate impact}. A dataset is said to have a disparate impact if the ratio of the probability of getting a positive outcome given the unprivileged group to the probability of getting a positive outcome given the privileged group is smaller than $0.8$. This metric is applied to the actual outcomes or predicted outcomes separately. 

The \emph{Predictive parity} metric considers the actual outcomes (original class labels) and compare them to the predicted outcomes to quantify fairness \cite{verma2018fairness_explained}. A classifier is considered fair if both unprivileged and privileged groups have the same Positive Predictive Value (PPV). \emph{Equalized odds} is similar to predictive parity, which deems a classifier fair when the true positive rates and the false positive rates for both unprivileged and privileged groups are equal \cite{zafar2017fairness}. A relaxed version of equalized odds is the \emph{equal opportunity} \cite{hardt2016equal_odds_opport}, which only considers one part of the definition of equalized odds. A classifier is fair by equal opportunity if it yields an equal true positive rate for both unprivileged and privileged groups. 


More statistical fairness metrics can also be found in the literature such as the \emph{overall accuracy equality} \cite{berk2018fairness}, which considers that true negative outcomes are as desirable as true positive outcomes \cite{berk2018fairness}. Another metric defined in \cite{berk2018fairness} is \emph{treatment equality}, which requires a classifier to produce equal false negative and false positive ratios. The treatment term is used to convey that these ratios can be a policy lever to achieve different kinds of fairness depending on the domain \cite{berk2018fairness}.


% The last type of statistical fairness metrics is based on both predicted probability scores calculated by a classifier and actual outcomes, or original labels, of a dataset. \emph{Test fairness}, or also named as calibration, is one of these metrics that requires a classifier to produce equal prediction probabilities for both protected and unprotected groups to truly belong to the positive class to be deemed as well-calibrated \cite{chouldechova2017fair}. Well-calibrated means that a classifier does not contain any predictive bias. It is a widely used metric as a standard for fairness assessment in the literature \cite{chouldechova2017fair}. Another metric in this called \emph{well-calibration}, which is an expanded version of the calibration metric. According to this metric, next to the equality conditions in calibration, the predicted probability value should also be equal to some value \emph{P}. This metric means that if a classifier finds that a set of samples in a dataset have a certain probability value \emph{p} of being assigned to the positive class, then also \emph{p} percentage of these samples (people) should originally have a positive class label. 

% The third fairness metric in this type is called \emph{the balance for positive class}, which deems a classifier fair if samples with the positive class label from both protected and unprotected groups have an equal predicted probability score \cite{kleinberg2017inherent}. \citeauthor{kleinberg2017inherent} have also formalized another fairness metric called \emph{the balance fore negative class} which is the opposite version of the previous fairness metric. This time, a classifier should provide an equal predicted probability score for the samples from both protected and unprotected groups constituting the negative class in a dataset. Thus, to satisfy the "balance for negative class" metric, people with negative class labels should have the same expected probability score, no matter what their sensitive attribute value is. It is important to note that these statistical metrics that require predicted probability scores can only be used with a limited number of classifiers that can calculate such scores, such as logistic regression or support vector machines.


% There are several more metrics defined in the literature, however, most of them have different names for the same metric formalization, as shown above with some examples. At the first glance, statistical fairness metrics seem very attractive due to their easy-to-understand nature. 

It should be noted that statistical fairness metrics cannot guarantee fairness for individuals or more fine-grained sub-groups of the unprivileged  groups \cite{chouldechova2018frontiers}. Furthermore, there is a disagreement among different statistical fairness metrics since their goals and the considered criteria are different, which is formalized and proven with the \emph{impossibility theorem} \cite{chouldechova2017fair,kleinberg2017inherent,pleiss2017calibration}. According to this theorem, it is impossible to satisfy both \emph{equalized odds} and  \emph{predictive parity} or calibration for a classifier simultaneously if the base rates of groups are different. 
% Only one of them can be satisfied at a time, unless the dataset has the same base rate for both protected and unprotected groups, which means that both groups have precisely the same number of samples with the positive class label, or unless the classifier in question is a perfect classifier which never makes any errors. Only in these two specific and rare cases, these three fairness metrics can be satisfied simultaneously. Due to all of these limitations mentioned above, researchers have come up with new fairness metrics that have different points of view. In the next sections, other proposed metrics that tackle these limitations and try to solve in respective papers are further explained.



\stitle{Individual fairness metrics:} This type of metrics consider the outcomes on the individual level. For example, the  \emph{fairness through unawareness} \cite{kusner2017counterfactual} and \emph{individual fairness} \cite{dwork2012fairness} are based on the fact that similar individuals should be treated similarly in classification tasks. \citeauthor{joseph2016fairness_qualified} in  \cite{joseph2016fairness_qualified} brought the \emph{contextual multi-armed bandit} problem to the fairness domain to ensure that any individual who has worse qualities than another individual will not be favored by the algorithm. According to \citeauthor{galhotra2017causal_discr} in \cite{galhotra2017causal_discr}, an algorithm is considered fair if it provides the same output for two individuals who have different values only in the sensitive attributes.
Individual fairness approaches show promising improvements for the domain of fairness. However, they have a fundamental limitation due to the assumption that the underlying distance metric for the given dataset is known. Furthermore, using a specific distance metric involve making assumptions regarding the relationship between the features and the labels in the dataset.

% \stitle{Sub-group Fairness Metrics}

Both statistical fairness metrics and individual fairness metrics have specific shortcomings that can be addressed by considering fairness metrics on the the sub-group level \cite{mehrabi2019survey}.
\citeauthor{kim2018fairness} \cite{kim2018fairness} introduce the \emph{metric multifairness} to treat the individuals in a given sub-population similarly, which can be considered as combining the individual and group fairness notions. 
\citeauthor{kearns2018preventing} \cite{kearns2018preventing} introduced \emph{fairness gerrymandering} to address the problem  when a classifier is fair on each group existing in a sensitive attribute but unfair for one or more subgroups in the dataset defined over multiple sensitive attributes. Although this approach is very promising since it does not need specific assumptions regarding the data just like the group-based or statistical metrics, one shortcoming of this approach is that it is not certain which function classes are feasible or reasonable to use for each dataset at hand, and there is no clear guidance about which attributes should be included as protected attributes to define the subgroups later \cite{chouldechova2018frontiers}.


% \stitle{Causal reasoning metrics:} investigating the causal relationships between the attributes and the outcome labels leads to the introduction of causal reasoning metrics. These metrics requires additional understanding and knowledge of how the world is structured in the form of a causal model \cite{loftus2018causal}. The required knowledge is substantial to understand how a change in an attribute can cause a change in the system. 
% \citeauthor{kusner2017counterfactual} \cite{kusner2017counterfactual} have proposed \emph{counterfactual fairness}, which considers a dataset as counterfactually fair if a decision regarding an individual is identical in the actual world as well as a counterfactual world where that individual belongs to a different demographic sub-population. Another counterfactual fairness metric was introduced by \citeauthor{kilbertus2017avoiding_discr} \cite{kilbertus2017avoiding_discr} that highlights the importance to distinguish the sensitive attributes from their related proxy attributes so that the underlying effects of sensitive attributes on the decision attribute can be revealed.  Causal reasoning metrics assume that sensitive and proxy attributes can be identified and resolved correctly and the appropriate causal graph for a given dataset can be also constructed accurately. 

\subsection{Mitigation Algorithms:}
Researchers have been not only working on finding the best metric but also working on finding an appropriate technique to eliminate the discrimination identified in a dataset or a model. Thus, there are several proposed bias mitigation techniques to eliminate or mitigate unfairness considering the accuracy performance.

\stitle{Pre-processing algorithms:} the first category of bias mitigation algorithms is \emph{pre-processing} algorithms, or techniques, where the dataset is altered before training a classifier in order to obtain a fair dataset as an input. \emph{Fairness through unawareness} is an example of this category, which considers a predictor model fair if none of the protected attributes are used in the prediction process \cite{gajane2017formalizing}. A more sophisticated approach to pre-process a dataset is re-sampling the data instances. \citeauthor{kamiran2010preferential_samp} \cite{kamiran2010preferential_samp} proposed the "preferential sampling" approach, where they sample the data objects with replacement in order to eliminate bias.  
\citeauthor{salimi2019capuchin} \cite{salimi2019capuchin} proposed "interventional fairness" where the training data is "repaired" by inserting or removing tuples and alter the probability distribution in the dataset in order to remove any causal relationship between sensitive attributes and the decision variable. 
\emph{Massaging} \cite{kamiran2012data-preproc} changes the actual class labels of some of the instances in the training set to ensure fairness. A ranker algorithm is used to choose the appropriate instances to relabel.
\citeauthor{feldman2015certifying} in \cite{feldman2015certifying} proposed using the massaging approach on the attributes (variables) other than the sensitive attribute(s) of a dataset. 

Another pre-processing technique uses \emph{reweighing} \cite{calders2009reweighing}, which assigns weights to each instance in the training set. Basically, this approach assigns higher weights to the instances from the unprivileged group with positive outcomes than the instances from the unprivileged group with negative outcome and vice versa. 

\citeauthor{zemel2013fair_learning} \cite{zemel2013fair_learning} proposed the \emph{learning fair representations} (LFR), where the fairness is defined as an optimization problem and the appropriate intermediate representation is found to encode the data as accurate as possible. Information about the sensitive attributes of individuals is concealed. 
A similar study to LFR \cite{zemel2013fair_learning} is conducted by \citeauthor{calmon2017optimized_pre} in \cite{calmon2017optimized_pre}, which also considers unfairness as an optimization problem with a probabilistic framework. However, test data are also transformed probabilistically before they are given to a classifier model as well as the training data in this pre-processing technique. 
\citeauthor{yan2020fair-balance} \cite{yan2020fair-balance} proposed a fair data oversampling technique called \emph{fair class balancing} to address the class imbalance problem in datasets.This approach does not use any information regarding the sensitive attributes.  


\stitle{In-Processing Algorithms:}
these algorithms tune or adjust the classification algorithm in order to make the model output fair. There are several classifiers that are altered for in-processing such as Support Vector Machines (SVM), logistic regression, and random forests. In processing algorithms are mostly limited to the chosen classifier. \citeauthor{kamiran2010decision-tree} in \cite{kamiran2010decision-tree} uses decision trees as a classifier that is  adjusted, or \emph{constrained}, to ensure fairness.
\citeauthor{zafar2017fairness-cons} \cite{zafar2017fairness-cons} implemented an in-processing algorithm based on constraining classifiers, which is formulated as a regularized optimization problem, using logistic regression and SVM algorithms. \citeauthor{kamishima2011fairness} \cite{kamishima2011fairness} proposed regularized prejudice remover, which can be applied on any probabilistic classifier to mitigate bias. The proposed technique enforces classifiers to make the predictions independent from a sensitive attribute. 
Adversarial learning is used as an in-processing technique to ensure fairness. \citeauthor{zhang2018adversarial} \cite{zhang2018adversarial} have proposed a framework with adversarial debiasing to mitigate bias, which can be implemented with gradient-based models for both classification and regression tasks. 
\citeauthor{ristanoski2013discr-aware} \cite{ristanoski2013discr-aware} proposed an empirical loss-based tuning on SVM, which also considers the imbalance in the number of samples with positive and negative class labels. 


\stitle{Post-Processing Algorithms:}
post-processing algorithms change the predicted outcomes of classifiers based on certain rules or constraints to ensure fairness. Thus, the goal is to eliminate the discrimination from the final predictions instead of the input dataset or within the models. \citeauthor{kamiran2012ROC} \cite{kamiran2012ROC} implemented \emph{reject option classification} (ROC) to change the predicted class labels of the instances that are close to the decision boundary. 
The ROC algorithm that \citeauthor{kamiran2012ROC} proposed can be named as a \emph{thresholding} technique since it considers a certain threshold and a critical region to modify the predicted outcomes of classifiers.
Another approach based on thresholding is proposed by \citeauthor{hardt2016equal_odds_opport} \cite{hardt2016equal_odds_opport}, where the equalized odds (EO) is used as the core fairness metric. The predictions of a classifier obtained at the end of the training step are adjusted to ensure the EO fairness. 


Finally, \citeauthor{kilbertus2017avoiding_discr} \cite{kilbertus2017avoiding_discr} have proposed two post-processing algorithms, namely avoiding proxy discrimination and avoiding unresolved discrimination, to eliminate unfairness on the predictions of a classifier based on causal perspective and two causal definitions. 
Unfortunately, most of the post-processing algorithms have a common limitations in practice. For example, the post-processing algorithms which make corrections on the classifier predictions by randomizing them cannot be used in specific domains due to ethical reasons. Furthermore, post-processing algorithms might deliver sub-optimal performance in terms of accuracy compared to the other fairness techniques \cite{woodworth2017post-learning}. 

% Thus, post-processing algorithms are not the best options for the practitioners who would like to achieve fairness while obtaining as high accuracy as possible.


