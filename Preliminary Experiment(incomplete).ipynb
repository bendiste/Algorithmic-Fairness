{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Experiments with Existing Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary experiment consists of three algorithms: learning fair representations (pre-processing), adversarial debiasing (in-processing), and calibrayed equalized odds (post-processing). In order to compare these algorithms, Adult census and German credit datasets will be used. Logistic regression classifier will be used to train and test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")  \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#datasets\n",
    "from aif360.datasets import AdultDataset, GermanDataset\n",
    "#functions to pre-process datasets \n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_german\n",
    "\n",
    "#metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "\n",
    "#algorithms\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "\n",
    "#progress meter for algorithms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#scalers & classifiers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#markdown and plotting\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the datassets\n",
    "\n",
    "#Adult\n",
    "adult_orig = load_preproc_data_adult()\n",
    "adult_orig_train, adult_orig_test = adult_orig.split([0.7], shuffle=True)\n",
    "\n",
    "a_privileged_groups = [{'sex': 1.0}]\n",
    "a_unprivileged_groups = [{'sex': 0.0}]\n",
    "\n",
    "#German credit\n",
    "german_orig = GermanDataset(\n",
    "    protected_attribute_names=['age'],          \n",
    "    privileged_classes=[lambda x: x >= 25],\n",
    "    # ignore sex-related attributes in order to focus on single binary sensitive attribute\n",
    "    features_to_drop=['personal_status', 'sex']\n",
    ")\n",
    "\n",
    "german_orig.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    if (german_orig.labels[i] == 2.):\n",
    "        german_orig.labels[i] = 0\n",
    "    else:\n",
    "        german_orig.labels[i] = 1\n",
    "        \n",
    "german_orig.favorable_label = 1\n",
    "german_orig.unfavorable_label = 0\n",
    "\n",
    "german_orig_train, german_orig_test = german_orig.split([0.7], shuffle=True)\n",
    "\n",
    "g_privileged_groups = [{'age': 1}]\n",
    "g_unprivileged_groups = [{'age': 0}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_orig.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "#It finda that German dataset has a class imbalance, 700 positive and 300 negative outcomes.\n",
    "k=0\n",
    "for i in range(1000):\n",
    "    if(german_orig.labels[i] == 1):\n",
    "        k+=1\n",
    "    else:\n",
    "        pass\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### German Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 57)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['month', 'credit_amount', 'investment_as_income_percentage', 'residence_since', 'age', 'number_of_credits', 'people_liable_for', 'status=A11', 'status=A12', 'status=A13', 'status=A14', 'credit_history=A30', 'credit_history=A31', 'credit_history=A32', 'credit_history=A33', 'credit_history=A34', 'purpose=A40', 'purpose=A41', 'purpose=A410', 'purpose=A42', 'purpose=A43', 'purpose=A44', 'purpose=A45', 'purpose=A46', 'purpose=A48', 'purpose=A49', 'savings=A61', 'savings=A62', 'savings=A63', 'savings=A64', 'savings=A65', 'employment=A71', 'employment=A72', 'employment=A73', 'employment=A74', 'employment=A75', 'other_debtors=A101', 'other_debtors=A102', 'other_debtors=A103', 'property=A121', 'property=A122', 'property=A123', 'property=A124', 'installment_plans=A141', 'installment_plans=A142', 'installment_plans=A143', 'housing=A151', 'housing=A152', 'housing=A153', 'skill_level=A171', 'skill_level=A172', 'skill_level=A173', 'skill_level=A174', 'telephone=A191', 'telephone=A192', 'foreign_worker=A201', 'foreign_worker=A202']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Adult Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34189, 18)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
     ]
    }
   ],
   "source": [
    "# some information of each dataset regarding labels, names, etc. \n",
    "display(Markdown(\"#### German Training Dataset shape\"))\n",
    "print(german_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(german_orig_train.favorable_label, german_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(german_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(german_orig_train.privileged_protected_attributes, \n",
    "      german_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(german_orig_train.feature_names)\n",
    "\n",
    "display(Markdown(\"#### Adult Training Dataset shape\"))\n",
    "print(adult_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(adult_orig_train.favorable_label, adult_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(adult_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(adult_orig_train.privileged_protected_attributes, \n",
    "      adult_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(adult_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights            features                       \\\n",
       "                                protected attribute                        \n",
       "                                               race  sex Age (decade)=10   \n",
       "instance names                                                             \n",
       "36846                       1.0                 1.0  1.0             0.0   \n",
       "17099                       1.0                 1.0  1.0             0.0   \n",
       "22225                       1.0                 1.0  1.0             0.0   \n",
       "27858                       1.0                 1.0  1.0             0.0   \n",
       "2199                        1.0                 1.0  1.0             0.0   \n",
       "...                         ...                 ...  ...             ...   \n",
       "17876                       1.0                 1.0  1.0             0.0   \n",
       "26648                       1.0                 1.0  1.0             0.0   \n",
       "40135                       1.0                 1.0  1.0             0.0   \n",
       "40043                       1.0                 1.0  1.0             0.0   \n",
       "43225                       1.0                 1.0  1.0             0.0   \n",
       "\n",
       "                                                                \\\n",
       "                                                                 \n",
       "               Age (decade)=20 Age (decade)=30 Age (decade)=40   \n",
       "instance names                                                   \n",
       "36846                      1.0             0.0             0.0   \n",
       "17099                      0.0             0.0             0.0   \n",
       "22225                      0.0             1.0             0.0   \n",
       "27858                      0.0             0.0             0.0   \n",
       "2199                       1.0             0.0             0.0   \n",
       "...                        ...             ...             ...   \n",
       "17876                      0.0             1.0             0.0   \n",
       "26648                      0.0             0.0             1.0   \n",
       "40135                      0.0             0.0             0.0   \n",
       "40043                      0.0             1.0             0.0   \n",
       "43225                      0.0             1.0             0.0   \n",
       "\n",
       "                                                                  \\\n",
       "                                                                   \n",
       "               Age (decade)=50 Age (decade)=60 Age (decade)=>=70   \n",
       "instance names                                                     \n",
       "36846                      0.0             0.0               0.0   \n",
       "17099                      0.0             1.0               0.0   \n",
       "22225                      0.0             0.0               0.0   \n",
       "27858                      1.0             0.0               0.0   \n",
       "2199                       0.0             0.0               0.0   \n",
       "...                        ...             ...               ...   \n",
       "17876                      0.0             0.0               0.0   \n",
       "26648                      0.0             0.0               0.0   \n",
       "40135                      1.0             0.0               0.0   \n",
       "40043                      0.0             0.0               0.0   \n",
       "43225                      0.0             0.0               0.0   \n",
       "\n",
       "                                                                      \\\n",
       "                                                                       \n",
       "               Education Years=6 Education Years=7 Education Years=8   \n",
       "instance names                                                         \n",
       "36846                        0.0               1.0               0.0   \n",
       "17099                        0.0               0.0               0.0   \n",
       "22225                        0.0               0.0               0.0   \n",
       "27858                        0.0               0.0               0.0   \n",
       "2199                         0.0               0.0               0.0   \n",
       "...                          ...               ...               ...   \n",
       "17876                        0.0               0.0               0.0   \n",
       "26648                        0.0               0.0               0.0   \n",
       "40135                        0.0               0.0               0.0   \n",
       "40043                        0.0               0.0               0.0   \n",
       "43225                        0.0               0.0               0.0   \n",
       "\n",
       "                                                                        \\\n",
       "                                                                         \n",
       "               Education Years=9 Education Years=10 Education Years=11   \n",
       "instance names                                                           \n",
       "36846                        0.0                0.0                0.0   \n",
       "17099                        1.0                0.0                0.0   \n",
       "22225                        0.0                0.0                0.0   \n",
       "27858                        0.0                0.0                0.0   \n",
       "2199                         1.0                0.0                0.0   \n",
       "...                          ...                ...                ...   \n",
       "17876                        1.0                0.0                0.0   \n",
       "26648                        0.0                1.0                0.0   \n",
       "40135                        0.0                0.0                0.0   \n",
       "40043                        0.0                1.0                0.0   \n",
       "43225                        1.0                0.0                0.0   \n",
       "\n",
       "                                                                          \\\n",
       "                                                                           \n",
       "               Education Years=12 Education Years=<6 Education Years=>12   \n",
       "instance names                                                             \n",
       "36846                         0.0                0.0                 0.0   \n",
       "17099                         0.0                0.0                 0.0   \n",
       "22225                         0.0                0.0                 1.0   \n",
       "27858                         0.0                0.0                 1.0   \n",
       "2199                          0.0                0.0                 0.0   \n",
       "...                           ...                ...                 ...   \n",
       "17876                         0.0                0.0                 0.0   \n",
       "26648                         0.0                0.0                 0.0   \n",
       "40135                         1.0                0.0                 0.0   \n",
       "40043                         0.0                0.0                 0.0   \n",
       "43225                         0.0                0.0                 0.0   \n",
       "\n",
       "               labels  \n",
       "                       \n",
       "                       \n",
       "instance names         \n",
       "36846             0.0  \n",
       "17099             0.0  \n",
       "22225             1.0  \n",
       "27858             1.0  \n",
       "2199              1.0  \n",
       "...               ...  \n",
       "17876             1.0  \n",
       "26648             1.0  \n",
       "40135             1.0  \n",
       "40043             0.0  \n",
       "43225             0.0  \n",
       "\n",
       "[34189 rows x 20 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_orig_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features                \\\n",
       "                                                         \n",
       "                                   month credit_amount   \n",
       "instance names                                           \n",
       "718                         1.0     24.0        3148.0   \n",
       "471                         1.0      6.0         448.0   \n",
       "765                         1.0     12.0        1155.0   \n",
       "353                         1.0     12.0        6199.0   \n",
       "709                         1.0      9.0        2118.0   \n",
       "...                         ...      ...           ...   \n",
       "234                         1.0      4.0        1544.0   \n",
       "202                         1.0     27.0        5117.0   \n",
       "776                         1.0     36.0        3535.0   \n",
       "181                         1.0     36.0        4455.0   \n",
       "300                         1.0      6.0         672.0   \n",
       "\n",
       "                                                                \\\n",
       "                                                                 \n",
       "               investment_as_income_percentage residence_since   \n",
       "instance names                                                   \n",
       "718                                        3.0             2.0   \n",
       "471                                        4.0             4.0   \n",
       "765                                        3.0             3.0   \n",
       "353                                        4.0             2.0   \n",
       "709                                        2.0             2.0   \n",
       "...                                        ...             ...   \n",
       "234                                        2.0             1.0   \n",
       "202                                        3.0             4.0   \n",
       "776                                        4.0             4.0   \n",
       "181                                        2.0             2.0   \n",
       "300                                        1.0             4.0   \n",
       "\n",
       "                                                                        \\\n",
       "               protected attribute                                       \n",
       "                               age number_of_credits people_liable_for   \n",
       "instance names                                                           \n",
       "718                            1.0               2.0               1.0   \n",
       "471                            0.0               1.0               1.0   \n",
       "765                            1.0               2.0               1.0   \n",
       "353                            1.0               2.0               1.0   \n",
       "709                            1.0               1.0               2.0   \n",
       "...                            ...               ...               ...   \n",
       "234                            1.0               3.0               2.0   \n",
       "202                            1.0               2.0               1.0   \n",
       "776                            1.0               2.0               1.0   \n",
       "181                            1.0               2.0               1.0   \n",
       "300                            1.0               1.0               1.0   \n",
       "\n",
       "                                      ...                                \\\n",
       "                                      ...                                 \n",
       "               status=A11 status=A12  ... housing=A153 skill_level=A171   \n",
       "instance names                        ...                                 \n",
       "718                   0.0        0.0  ...          0.0              0.0   \n",
       "471                   1.0        0.0  ...          0.0              0.0   \n",
       "765                   0.0        1.0  ...          0.0              0.0   \n",
       "353                   1.0        0.0  ...          0.0              0.0   \n",
       "709                   0.0        1.0  ...          0.0              0.0   \n",
       "...                   ...        ...  ...          ...              ...   \n",
       "234                   0.0        0.0  ...          0.0              0.0   \n",
       "202                   0.0        0.0  ...          0.0              0.0   \n",
       "776                   0.0        0.0  ...          0.0              0.0   \n",
       "181                   0.0        1.0  ...          0.0              0.0   \n",
       "300                   0.0        0.0  ...          0.0              1.0   \n",
       "\n",
       "                                                                   \\\n",
       "                                                                    \n",
       "               skill_level=A172 skill_level=A173 skill_level=A174   \n",
       "instance names                                                      \n",
       "718                         0.0              1.0              0.0   \n",
       "471                         0.0              1.0              0.0   \n",
       "765                         1.0              0.0              0.0   \n",
       "353                         0.0              1.0              0.0   \n",
       "709                         1.0              0.0              0.0   \n",
       "...                         ...              ...              ...   \n",
       "234                         1.0              0.0              0.0   \n",
       "202                         0.0              1.0              0.0   \n",
       "776                         0.0              1.0              0.0   \n",
       "181                         0.0              0.0              1.0   \n",
       "300                         0.0              0.0              0.0   \n",
       "\n",
       "                                                                  \\\n",
       "                                                                   \n",
       "               telephone=A191 telephone=A192 foreign_worker=A201   \n",
       "instance names                                                     \n",
       "718                       0.0            1.0                 1.0   \n",
       "471                       1.0            0.0                 1.0   \n",
       "765                       1.0            0.0                 1.0   \n",
       "353                       0.0            1.0                 1.0   \n",
       "709                       1.0            0.0                 1.0   \n",
       "...                       ...            ...                 ...   \n",
       "234                       1.0            0.0                 1.0   \n",
       "202                       1.0            0.0                 1.0   \n",
       "776                       0.0            1.0                 1.0   \n",
       "181                       0.0            1.0                 1.0   \n",
       "300                       0.0            1.0                 1.0   \n",
       "\n",
       "                                   labels  \n",
       "                                           \n",
       "               foreign_worker=A202         \n",
       "instance names                             \n",
       "718                            0.0    1.0  \n",
       "471                            0.0    0.0  \n",
       "765                            0.0    1.0  \n",
       "353                            0.0    0.0  \n",
       "709                            0.0    1.0  \n",
       "...                            ...    ...  \n",
       "234                            0.0    1.0  \n",
       "202                            0.0    1.0  \n",
       "776                            0.0    1.0  \n",
       "181                            0.0    0.0  \n",
       "300                            0.0    1.0  \n",
       "\n",
       "[700 rows x 59 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_orig_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Adult original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact (of original labels) between unprivileged and privileged groups = 0.349001\n",
      "Difference in statistical parity (of original labels) between unprivileged and privileged groups = -0.198855\n",
      "Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = 0.739776\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Adult original test dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact (of original labels) between unprivileged and privileged groups = 0.384917\n",
      "Difference in statistical parity (of original labels) between unprivileged and privileged groups = -0.184405\n",
      "Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = 0.738948\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### German original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of original labels) between unprivileged and privileged groups = 0.782540\n",
      "Difference in statistical parity (of original labels) between unprivileged and privileged groups = -0.153501\n",
      "Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = 0.666286\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### German original test dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of original labels) between unprivileged and privileged groups = 0.909091\n",
      "Difference in statistical parity (of original labels) between unprivileged and privileged groups = -0.068182\n",
      "Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = 0.730000\n"
     ]
    }
   ],
   "source": [
    "# Initial disparities in the original datasets\n",
    "\n",
    "#Adult\n",
    "metric_ad_orig_train = BinaryLabelDatasetMetric(adult_orig_train, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "display(Markdown(\"#### Adult original training dataset\"))\n",
    "\n",
    "print(\"Disparate impact (of original labels) between unprivileged and privileged groups = %f\" % metric_ad_orig_train.disparate_impact())\n",
    "print(\"Difference in statistical parity (of original labels) between unprivileged and privileged groups = %f\" % metric_ad_orig_train.statistical_parity_difference())\n",
    "print(\"Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = %f\" % metric_ad_orig_train.consistency())\n",
    "\n",
    "metric_ad_orig_test = BinaryLabelDatasetMetric(adult_orig_test, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "display(Markdown(\"#### Adult original test dataset\"))\n",
    "\n",
    "print(\"Disparate impact (of original labels) between unprivileged and privileged groups = %f\" % metric_ad_orig_test.disparate_impact())\n",
    "print(\"Difference in statistical parity (of original labels) between unprivileged and privileged groups = %f\" % metric_ad_orig_test.statistical_parity_difference())\n",
    "print(\"Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = %f\" % metric_ad_orig_test.consistency())\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "#German\n",
    "metric_ger_orig_train = BinaryLabelDatasetMetric(german_orig_train, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "display(Markdown(\"#### German original training dataset\"))\n",
    "\n",
    "print(\"Disparate impact ratio (of original labels) between unprivileged and privileged groups = %f\" % metric_ger_orig_train.disparate_impact())\n",
    "print(\"Difference in statistical parity (of original labels) between unprivileged and privileged groups = %f\" % metric_ger_orig_train.statistical_parity_difference())\n",
    "print(\"Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = %f\" % metric_ger_orig_train.consistency())\n",
    "\n",
    "metric_ger_orig_test = BinaryLabelDatasetMetric(german_orig_test, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "display(Markdown(\"#### German original test dataset\"))\n",
    "\n",
    "print(\"Disparate impact ratio (of original labels) between unprivileged and privileged groups = %f\" % metric_ger_orig_test.disparate_impact())\n",
    "print(\"Difference in statistical parity (of original labels) between unprivileged and privileged groups = %f\" % metric_ger_orig_test.statistical_parity_difference())\n",
    "print(\"Individual fairness metric from Zemel et.al. that measures how similar the labels are for similar instances = %f\" % metric_ger_orig_test.consistency())\n",
    "\n",
    "#n_neighbors warning exists due to how this individual metric is implemented in the library. The function has its own parameter n_neighbors=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial fairness performance of the predictions of a classifier without mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the dataset\n",
    "scale_orig = StandardScaler()\n",
    "\n",
    "#German\n",
    "X_train_g = scale_orig.fit_transform(german_orig_train.features)\n",
    "X_test_g = scale_orig.transform(german_orig_test.features)\n",
    "\n",
    "y_train_g = german_orig_train.labels.ravel()\n",
    "y_test_g = german_orig_test.labels.ravel()\n",
    "\n",
    "#Adult\n",
    "X_train_a = scale_orig.fit_transform(adult_orig_train.features)\n",
    "X_test_a = scale_orig.transform(adult_orig_test.features)\n",
    "\n",
    "y_train_a = adult_orig_train.labels.ravel()\n",
    "y_test_a = adult_orig_test.labels.ravel()\n",
    "\n",
    "\n",
    "#Logistic Regression Training for each dataset\n",
    "log_reg_g = LogisticRegression(class_weight='balanced', solver='liblinear') \n",
    "log_reg_a = LogisticRegression(class_weight='balanced', solver='liblinear') \n",
    "\n",
    "#Fitting the German dataset\n",
    "log_reg_g.fit(X_train_g, y_train_g)\n",
    "\n",
    "#Fitting Adult dataset\n",
    "log_reg_a.fit(X_train_a, y_train_a)\n",
    "\n",
    "#Predicting test set labels\n",
    "y_test_pred_g = log_reg_g.predict(X_test_g)\n",
    "y_test_pred_proba_g = log_reg_g.predict_proba(X_test_g)\n",
    "\n",
    "y_test_pred_a = log_reg_a.predict(X_test_a)\n",
    "y_test_pred_proba_a = log_reg_a.predict_proba(X_test_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_pred_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Performance of the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### German Test Set Fairness Performance Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average equalized odds difference between unprivileged and privileged groups = -0.207217\n",
      "Disparate impact ratio between unprivileged and privileged groups = 0.599449\n",
      "Demographic parity difference between unprivileged and privileged groups = -0.258168\n",
      "Predictive Parity difference between unprivileged and privileged groups = -0.002496\n",
      "Consistency of indivuals' predicted labels = 0.638667\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Adult Test Set Fairness Performance Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average equalized odds difference between unprivileged and privileged groups = -0.309830\n",
      "Disparate impact ratio between unprivileged and privileged groups = 0.303623\n",
      "Demographic parity difference between unprivileged and privileged groups = -0.354631\n",
      "Predictive Parity difference between unprivileged and privileged groups = -0.137221\n",
      "Consistency of indivuals' predicted labels = 0.999126\n"
     ]
    }
   ],
   "source": [
    "#1) German Dataset\n",
    "\n",
    "display(Markdown(\"#### German Test Set Fairness Performance Results\"))\n",
    "\n",
    "#Create a new version of the test set with predicted class labels\n",
    "g_testset_pred = german_orig_test.copy()\n",
    "g_testset_pred.labels = y_test_pred_g\n",
    "\n",
    "#Construction 1\n",
    "#to construct this metric function, the predicted labels should be united with the test fetures to make a new datas\n",
    "metric_ger_pred_test = BinaryLabelDatasetMetric(g_testset_pred, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "\n",
    "#Construction 2\n",
    "#both original test dataset with actual labels and the test dataset combined with predicted class labels need to be given to this function\n",
    "classified_metric_g = ClassificationMetric(german_orig_test, \n",
    "                                                 g_testset_pred,\n",
    "                                                 unprivileged_groups=g_unprivileged_groups,\n",
    "                                                 privileged_groups=g_privileged_groups)\n",
    "\n",
    "\n",
    "#Checking Equalized Odds: average odds differecence, which is the avg. of differences in FPR&TPR for privileged and unprivileged groups.\n",
    "aeo_g = classified_metric_g.average_odds_difference()\n",
    "print(\"Average equalized odds difference between unprivileged and privileged groups = %f\" % aeo_g)\n",
    "\n",
    "#Disparate Impact ratio between privileged and unprivileged groups.\n",
    "di_g = classified_metric_g.disparate_impact()\n",
    "print(\"Disparate impact ratio between unprivileged and privileged groups = %f\" % di_g)\n",
    "\n",
    "#Demographic parity difference between privileged and unprivileged groups.\n",
    "spd_g = classified_metric_g.statistical_parity_difference()\n",
    "print(\"Demographic parity difference between unprivileged and privileged groups = %f\" % spd_g)\n",
    "\n",
    "#Predictive parity difference: PPV difference between privileged and unprivileged groups.\n",
    "ppd_g = classified_metric_g.positive_predictive_value(privileged=False) - classified_metric_g.positive_predictive_value(privileged=True)\n",
    "print(\"Predictive Parity difference between unprivileged and privileged groups = %f\" % ppd_g)\n",
    "\n",
    "#Individual Fairness: 1)Consistency, 2) Euclidean Distance between individuals.\n",
    "print(\"Consistency of indivuals' predicted labels = %f\" % metric_ger_pred_test.consistency())\n",
    "\n",
    "\n",
    "\n",
    "#2) Adult Dataset\n",
    "display(Markdown(\"#### Adult Test Set Fairness Performance Results\"))\n",
    "\n",
    "a_testset_pred = adult_orig_test.copy()\n",
    "a_testset_pred.labels = y_test_pred_a\n",
    "\n",
    "#Construction 1\n",
    "#to construct this metric function, the predicted labels should be united with the test fetures to make a new datas\n",
    "metric_ad_pred_test = BinaryLabelDatasetMetric(a_testset_pred, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "\n",
    "\n",
    "#Construction 2\n",
    "#both original test dataset and the test dataset with predicted class labels need to be given to this function\n",
    "classified_metric_a = ClassificationMetric(adult_orig_test, \n",
    "                                                 a_testset_pred,\n",
    "                                                 unprivileged_groups=a_unprivileged_groups,\n",
    "                                                 privileged_groups=a_privileged_groups)\n",
    "\n",
    "#Checking Equalized Odds: average odds differecence, which is the avg. of differences in FPR&TPR for privileged and unprivileged groups.\n",
    "aeo_a = classified_metric_a.average_odds_difference()\n",
    "print(\"Average equalized odds difference between unprivileged and privileged groups = %f\" % aeo_a)\n",
    "\n",
    "#Disparate Impact ratio between privileged and unprivileged groups.\n",
    "di_a = classified_metric_a.disparate_impact()\n",
    "print(\"Disparate impact ratio between unprivileged and privileged groups = %f\" % di_a)\n",
    "\n",
    "#Demographic parity difference between privileged and unprivileged groups.\n",
    "spd_a = classified_metric_a.statistical_parity_difference()\n",
    "print(\"Demographic parity difference between unprivileged and privileged groups = %f\" % spd_a)\n",
    "\n",
    "#Predictive parity difference: PPV difference between privileged and unprivileged groups.\n",
    "ppd_a = classified_metric_a.positive_predictive_value(privileged=False) - classified_metric_a.positive_predictive_value(privileged=True)\n",
    "print(\"Predictive Parity difference between unprivileged and privileged groups = %f\" % ppd_a)\n",
    "\n",
    "#Individual Fairness: 1)Consistency, 2) Euclidean Distance between individuals.\n",
    "print(\"Consistency of indivuals' predicted labels = %f\" % metric_ad_pred_test.consistency())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Performance Metrics Before Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Classifier Prediction Performance on German Test Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy of logistic regression trained on German dataset without any mitigation = 0.726667\n",
      "Balanced accuracy of logistic regression trained on German dataset without any mitigation = 0.727997\n",
      "F1 score of logistic regression trained on German dataset without any mitigation = 0.797030\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classifier Prediction Performance on Adult Test Set "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy of logistic regression trained on German dataset without any mitigation = 0.730567\n",
      "Balanced accuracy of logistic regression trained on German dataset without any mitigation = 0.739181\n",
      "F1 score of logistic regression trained on German dataset without any mitigation = 0.572264\n"
     ]
    }
   ],
   "source": [
    "#German\n",
    "\n",
    "TPRg = classified_metric_g.true_positive_rate() #recall\n",
    "TNRg = classified_metric_g.true_negative_rate() #specificity\n",
    "PPVg = classified_metric_g.positive_predictive_value() #precision\n",
    "bal_acc_g = (TPRg+TNRg)/2\n",
    "f1_g = 2*((PPVg*TPRg)/(PPVg+TPRg))\n",
    "\n",
    "display(Markdown(\"#### Classifier Prediction Performance on German Test Set\"))\n",
    "print(\"Standard accuracy of logistic regression trained on German dataset without any mitigation = %f\" % classified_metric_g.accuracy())\n",
    "print(\"Balanced accuracy of logistic regression trained on German dataset without any mitigation = %f\" % bal_acc_g)\n",
    "print(\"F1 score of logistic regression trained on German dataset without any mitigation = %f\" % f1_g)\n",
    "\n",
    "\n",
    "#Adult\n",
    "\n",
    "TPRa = classified_metric_a.true_positive_rate()\n",
    "TNRa = classified_metric_a.true_negative_rate()\n",
    "PPVa = classified_metric_a.positive_predictive_value()\n",
    "bal_acc_a = (TPRa+TNRa)/2\n",
    "f1_a = 2*((PPVa*TPRa)/(PPVa+TPRa))\n",
    "\n",
    "display(Markdown(\"#### Classifier Prediction Performance on Adult Test Set \"))\n",
    "print(\"Standard accuracy of logistic regression trained on German dataset without any mitigation = %f\" % classified_metric_a.accuracy())\n",
    "print(\"Balanced accuracy of logistic regression trained on German dataset without any mitigation = %f\" % bal_acc_a)\n",
    "print(\"F1 score of logistic regression trained on German dataset without any mitigation = %f\" % f1_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 57  21]\n",
      " [ 61 161]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm_german = confusion_matrix(german_orig_test.labels, g_testset_pred.labels)\n",
    "\n",
    "disp_german = ConfusionMatrixDisplay(confusion_matrix=cm_german,\n",
    "                              display_labels=log_reg_g.classes_)\n",
    "disp_german.plot() \n",
    "print(cm_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8064 3094]\n",
      " [ 854 2641]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_adult = confusion_matrix(adult_orig_test.labels, a_testset_pred.labels)\n",
    "\n",
    "disp_adult = ConfusionMatrixDisplay(confusion_matrix=cm_adult,\n",
    "                              display_labels=log_reg_a.classes_)\n",
    "disp_adult.plot() \n",
    "print(cm_adult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the Mitigation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing algorithm: Learning Fair Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 90306.36003896409\n",
      "500 80078.5328275418\n",
      "750 79120.46611630426\n",
      "1000 77046.55851525687\n",
      "1250 74577.57813132436\n",
      "1500 70275.22338106718\n",
      "1750 68694.4809387689\n",
      "2000 69503.3259307258\n",
      "2250 72486.29062741545\n",
      "2500 66275.8441892547\n",
      "2750 65021.28609418889\n",
      "3000 64091.14101906252\n",
      "3250 63320.85375665051\n",
      "3500 62165.80127457388\n",
      "3750 61493.348339243385\n",
      "4000 60327.36163417848\n",
      "4250 59671.09415112863\n",
      "4500 59380.55507053173\n",
      "4750 59075.49481090723\n",
      "5000 58302.25508954917\n"
     ]
    }
   ],
   "source": [
    "#1) Transforming Adult Dataset\n",
    "\n",
    "#Required Inputs:\n",
    "# Input recontruction quality - Ax\n",
    "# Fairness constraint - Az\n",
    "# Output prediction error - Ay\n",
    "\n",
    "#scaled dataset together with its labels is needed\n",
    "adult_orig_train.features = scale_orig.fit_transform(adult_orig_train.features)\n",
    "adult_orig_test.features = scale_orig.transform(adult_orig_test.features)\n",
    "\n",
    "#LFR itself contains logistic regression sinc it uses signoid functions \n",
    "LFR_a =LFR(unprivileged_groups=a_unprivileged_groups,\n",
    "         privileged_groups=a_privileged_groups,\n",
    "         k=10, Ax=0.1, Ay=1.0, Az=2.0,\n",
    "         verbose=1\n",
    "        )\n",
    "TR_a = LFR_a.fit(adult_orig_train, maxiter=5000, maxfun=5000)\n",
    "\n",
    "\n",
    "# Transform training data and align features\n",
    "a_transf_train = TR_a.transform(adult_orig_train)\n",
    "a_transf_test = TR_a.transform(adult_orig_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5250 5174.340256982482\n",
      "5500 5174.340204508332\n",
      "5750 5174.34020742048\n",
      "6000 4933.20266266008\n",
      "6250 4933.202669127981\n",
      "6500 4933.202689967512\n",
      "6750 4367.433633185673\n",
      "7000 4367.433626513126\n",
      "7250 4367.43382716014\n",
      "7500 4486.82089127534\n",
      "7750 4486.820880235935\n",
      "8000 4486.821021105483\n",
      "8250 4269.803911780465\n",
      "8500 4269.803904453566\n",
      "8750 4252.427878845078\n",
      "9000 4252.427894885917\n",
      "9250 4252.427927193267\n",
      "9500 4222.301270607914\n",
      "9750 4222.301278707445\n",
      "10000 4222.301273083467\n",
      "10250 4141.5158664988285\n",
      "10500 4141.5157374169485\n",
      "10750 4141.515928131614\n",
      "11000 4147.616583238368\n",
      "11250 4147.616576100433\n"
     ]
    }
   ],
   "source": [
    "#2) Transforming German Dataset\n",
    "\n",
    "#scaled dataset together with its labels is needed\n",
    "german_orig_train.features = scale_orig.fit_transform(german_orig_train.features)\n",
    "german_orig_test.features = scale_orig.transform(german_orig_test.features)\n",
    "\n",
    "#LFR itself contains logistic regression sinc it uses signoid functions \n",
    "LFR_g =LFR(unprivileged_groups=g_unprivileged_groups,\n",
    "         privileged_groups=g_privileged_groups,\n",
    "         k=10, Ax=0.1, Ay=1.0, Az=2.0,\n",
    "         verbose=1\n",
    "        )\n",
    "TR_g = LFR_g.fit(german_orig_train, maxiter=5000, maxfun=5000)\n",
    "\n",
    "\n",
    "# Transform training data and align features\n",
    "g_transf_train = TR_g.transform(german_orig_train)\n",
    "g_transf_test = TR_g.transform(german_orig_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation of LFR algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness performance of the datasets before classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed Adult train set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of transformed labels) between unprivileged and privileged groups = 0.000000\n",
      "Difference in statistical parity (of transformed data) between unprivileged and privileged groups = -0.220624\n",
      "Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = 1.000000\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Transformed Adult test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of transformed data) between unprivileged and privileged groups = 0.000000\n",
      "Difference in statistical parity (of transformed data) between unprivileged and privileged groups = -0.227435\n",
      "Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = 0.999945\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Transformed German train set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of transformed data) between unprivileged and privileged groups = 1.011190\n",
      "Difference in statistical parity (of transformed data) between unprivileged and privileged groups = 0.010644\n",
      "Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = 0.987429\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Transformed German test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact ratio (of transformed data) between unprivileged and privileged groups = 1.016999\n",
      "Difference in statistical parity (of transformed data) between unprivileged and privileged groups = 0.016335\n",
      "Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = 0.984667\n"
     ]
    }
   ],
   "source": [
    "#Fairness Performance of Datasets Before Classification\n",
    "\n",
    "#Constucting two functions to call the desired metrics\n",
    "#Adult\n",
    "metric_transf_train_a = BinaryLabelDatasetMetric(a_transf_train, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed Adult train set\"))\n",
    "print(\"Disparate impact ratio (of transformed labels) between unprivileged and privileged groups = %f\" % metric_transf_train_a.disparate_impact())\n",
    "print(\"Difference in statistical parity (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_train_a.statistical_parity_difference())\n",
    "print(\"Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = %f\" % metric_transf_train_a.consistency())\n",
    "\n",
    "\n",
    "metric_transf_test_a = BinaryLabelDatasetMetric(a_transf_test, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed Adult test set\"))\n",
    "print(\"Disparate impact ratio (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_test_a.disparate_impact())\n",
    "print(\"Difference in statistical parity (of transformed data) between unprivileged and privileged groups = %f\" %metric_transf_test_a.statistical_parity_difference())\n",
    "print(\"Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = %f\" % metric_transf_test_a.consistency())\n",
    "\n",
    "\n",
    "#German\n",
    "metric_transf_train_g = BinaryLabelDatasetMetric(g_transf_train, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed German train set\"))\n",
    "print(\"Disparate impact ratio (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_train_g.disparate_impact())\n",
    "print(\"Difference in statistical parity (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_train_g.statistical_parity_difference())\n",
    "print(\"Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = %f\" % metric_transf_train_g.consistency())\n",
    "\n",
    "\n",
    "metric_transf_test_g = BinaryLabelDatasetMetric(g_transf_test, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Transformed German test set\"))\n",
    "print(\"Disparate impact ratio (of transformed data) between unprivileged and privileged groups = %f\" % metric_transf_test_g.disparate_impact())\n",
    "print(\"Difference in statistical parity (of transformed data) between unprivileged and privileged groups = %f\" %metric_transf_test_g.statistical_parity_difference())\n",
    "print(\"Individual fairness metric 'consistency' that measures how similar the labels are for similar instances = %f\" % metric_transf_test_g.consistency())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness and predictive performance after classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fairness Performance of Predictions After Classification with Logistic Regression\n",
    "#Note: this time scaling is not applied since the original datasets were already scaled before transformation.\n",
    "#German\n",
    "X_train_g_trans =g_transf_train.features\n",
    "X_test_g_trans = g_transf_test.features\n",
    "\n",
    "y_train_g_trans = g_transf_train.labels.ravel()\n",
    "y_test_g_trans = g_transf_test.labels.ravel()\n",
    "\n",
    "#Adult\n",
    "X_train_a_trans = a_transf_train.features\n",
    "X_test_a_trans = a_transf_test.features\n",
    "\n",
    "y_train_a_trans = a_transf_train.labels.ravel()\n",
    "y_test_a_trans = a_transf_test.labels.ravel()\n",
    "\n",
    "\n",
    "#Logistic Regression Training for each dataset\n",
    "trans_lr_g = LogisticRegression(class_weight='balanced', solver='liblinear') \n",
    "trans_lr_a = LogisticRegression(class_weight='balanced', solver='liblinear') \n",
    "\n",
    "#Fitting the German dataset\n",
    "trans_lr_g.fit(X_train_g_trans, y_train_g_trans)\n",
    "\n",
    "#Fitting Adult dataset\n",
    "trans_lr_a.fit(X_train_a_trans, y_train_a_trans)\n",
    "\n",
    "#Predicting test set labels\n",
    "y_test_trans_pred_g = log_reg_g.predict(X_test_g_trans)\n",
    "y_test_trans_pred_proba_g = log_reg_g.predict_proba(X_test_g_trans)\n",
    "\n",
    "y_test_trans_pred_a = log_reg_a.predict(X_test_a_trans)\n",
    "y_test_trans_pred_proba_a = log_reg_a.predict_proba(X_test_a_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### German Transformed Test Set Fairness Performance (based on predictions)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average equalized odds difference between unprivileged and privileged groups = -0.046833\n",
      "Disparate impact ratio between unprivileged and privileged groups = 1.019425\n",
      "Demographic parity difference between unprivileged and privileged groups = 0.017756\n",
      "Predictive Parity difference between unprivileged and privileged groups = 0.004274\n",
      "Consistency of indivuals' predicted labels = 0.989333\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Adult Transformed Test Set Fairness Performance (based on predictions)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average equalized odds difference between unprivileged and privileged groups = nan\n",
      "Disparate impact ratio between unprivileged and privileged groups = 0.770695\n",
      "Demographic parity difference between unprivileged and privileged groups = -0.073130\n",
      "Predictive Parity difference between unprivileged and privileged groups = -0.696795\n",
      "Consistency of indivuals' predicted labels = 0.999836\n"
     ]
    }
   ],
   "source": [
    "#Constructors to retrieve the results\n",
    "\n",
    "#1) German Dataset\n",
    "\n",
    "display(Markdown(\"#### German Transformed Test Set Fairness Performance (based on predictions)\"))\n",
    "\n",
    "#Create a new version of the transformed test set with predicted class labels\n",
    "g_testset_pred_trans = g_transf_test.copy()\n",
    "g_testset_pred_trans.labels = y_test_trans_pred_g\n",
    "\n",
    "#Construction 1\n",
    "#to construct this metric function, the predicted labels should be united with the test fetures to make a new datas\n",
    "metric_ger_pred_trans_test = BinaryLabelDatasetMetric(g_testset_pred_trans, \n",
    "                                             unprivileged_groups=g_unprivileged_groups,\n",
    "                                             privileged_groups=g_privileged_groups)\n",
    "\n",
    "#Construction 2\n",
    "'''both transformed test dataset with actual labels and the transformed test dataset combined with predicted class \n",
    "labels need to be given to this function'''\n",
    "classified_metric_trans_g = ClassificationMetric(g_transf_test, \n",
    "                                                 g_testset_pred_trans,\n",
    "                                                 unprivileged_groups=g_unprivileged_groups,\n",
    "                                                 privileged_groups=g_privileged_groups)\n",
    "\n",
    "\n",
    "#Checking Equalized Odds: average odds differecence, which is the avg. of differences in FPR&TPR for privileged and unprivileged groups.\n",
    "t_aeo_g = classified_metric_trans_g.average_odds_difference()\n",
    "print(\"Average equalized odds difference between unprivileged and privileged groups = %f\" % t_aeo_g)\n",
    "\n",
    "#Disparate Impact ratio between privileged and unprivileged groups.\n",
    "t_di_g = classified_metric_trans_g.disparate_impact()\n",
    "print(\"Disparate impact ratio between unprivileged and privileged groups = %f\" % t_di_g)\n",
    "\n",
    "#Demographic parity difference between privileged and unprivileged groups.\n",
    "t_spd_g = classified_metric_trans_g.statistical_parity_difference()\n",
    "print(\"Demographic parity difference between unprivileged and privileged groups = %f\" % t_spd_g)\n",
    "\n",
    "#Predictive parity difference: PPV difference between privileged and unprivileged groups.\n",
    "t_ppd_g = classified_metric_trans_g.positive_predictive_value(privileged=False) - classified_metric_trans_g.positive_predictive_value(privileged=True)\n",
    "print(\"Predictive Parity difference between unprivileged and privileged groups = %f\" % t_ppd_g)\n",
    "\n",
    "#Individual Fairness: 1)Consistency, 2) Euclidean Distance between individuals.\n",
    "print(\"Consistency of indivuals' predicted labels = %f\" % metric_ger_pred_trans_test.consistency())\n",
    "\n",
    "\n",
    "\n",
    "#2) Adult Dataset\n",
    "display(Markdown(\"#### Adult Transformed Test Set Fairness Performance (based on predictions)\"))\n",
    "\n",
    "#Create a new version of the transformed test set with predicted class labels\n",
    "a_testset_pred_trans = a_transf_test.copy()\n",
    "a_testset_pred_trans.labels = y_test_trans_pred_a\n",
    "\n",
    "#Construction 1\n",
    "#to construct this metric function, the predicted labels should be united with the test fetures to make a new datas\n",
    "metric_ad_pred_trans_test = BinaryLabelDatasetMetric(a_testset_pred_trans, \n",
    "                                             unprivileged_groups=a_unprivileged_groups,\n",
    "                                             privileged_groups=a_privileged_groups)\n",
    "\n",
    "\n",
    "#Construction 2\n",
    "#both original test dataset and the test dataset with predicted class labels need to be given to this function\n",
    "classified_metric_trans_a = ClassificationMetric(a_transf_test, \n",
    "                                                 a_testset_pred_trans,\n",
    "                                                 unprivileged_groups=a_unprivileged_groups,\n",
    "                                                 privileged_groups=a_privileged_groups)\n",
    "\n",
    "#Checking Equalized Odds: average odds differecence, which is the avg. of differences in FPR&TPR for privileged and unprivileged groups.\n",
    "t_aeo_a = classified_metric_trans_a.average_odds_difference()\n",
    "print(\"Average equalized odds difference between unprivileged and privileged groups = %f\" % t_aeo_a)\n",
    "\n",
    "#Disparate Impact ratio between privileged and unprivileged groups.\n",
    "t_di_a = classified_metric_trans_a.disparate_impact()\n",
    "print(\"Disparate impact ratio between unprivileged and privileged groups = %f\" % t_di_a)\n",
    "\n",
    "#Demographic parity difference between privileged and unprivileged groups.\n",
    "t_spd_a = classified_metric_trans_a.statistical_parity_difference()\n",
    "print(\"Demographic parity difference between unprivileged and privileged groups = %f\" % t_spd_a)\n",
    "\n",
    "#Predictive parity difference: PPV difference between privileged and unprivileged groups.\n",
    "t_ppd_a = classified_metric_trans_a.positive_predictive_value(privileged=False) - classified_metric_trans_a.positive_predictive_value(privileged=True)\n",
    "print(\"Predictive Parity difference between unprivileged and privileged groups = %f\" % t_ppd_a)\n",
    "\n",
    "#Individual Fairness: 1)Consistency, 2) Euclidean Distance between individuals.\n",
    "print(\"Consistency of indivuals' predicted labels = %f\" % metric_ad_pred_trans_test.consistency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Classifier Prediction Performance on Transformed German Test Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy of logistic regression trained on German dataset with LFR mitigation = 0.946667\n",
      "Balanced accuracy of logistic regression trained on German dataset with LFR mitigation = 0.928594\n",
      "F1 score of logistic regression trained on German dataset with LFR mitigation = 0.971631\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Classifier Prediction Performance on Transformed Adult Test Set "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy of logistic regression trained on German dataset with LFR mitigation = 0.850270\n",
      "Balanced accuracy of logistic regression trained on German dataset with LFRy mitigation = 0.902323\n",
      "F1 score of logistic regression trained on German dataset with LFR mitigation = 0.664629\n"
     ]
    }
   ],
   "source": [
    "#classifier performance\n",
    "\n",
    "#German\n",
    "\n",
    "TPRg_tr = classified_metric_trans_g.true_positive_rate() #recall\n",
    "TNRg_tr = classified_metric_trans_g.true_negative_rate() #specificity\n",
    "PPVg_tr = classified_metric_trans_g.positive_predictive_value() #precision\n",
    "bal_acc_g_tr = (TPRg_tr+TNRg_tr)/2\n",
    "f1_g_tr = 2*((PPVg_tr*TPRg_tr)/(PPVg_tr+TPRg_tr))\n",
    "\n",
    "display(Markdown(\"#### Classifier Prediction Performance on Transformed German Test Set\"))\n",
    "print(\"Standard accuracy of logistic regression trained on German dataset with LFR mitigation = %f\" % classified_metric_trans_g.accuracy())\n",
    "print(\"Balanced accuracy of logistic regression trained on German dataset with LFR mitigation = %f\" % bal_acc_g_tr)\n",
    "print(\"F1 score of logistic regression trained on German dataset with LFR mitigation = %f\" % f1_g_tr)\n",
    "\n",
    "#Adult\n",
    "\n",
    "TPRa_tr = classified_metric_trans_a.true_positive_rate()\n",
    "TNRa_tr = classified_metric_trans_a.true_negative_rate()\n",
    "PPVa_tr = classified_metric_trans_a.positive_predictive_value()\n",
    "bal_acc_a_tr = (TPRa_tr+TNRa_tr)/2\n",
    "f1_a_tr = 2*((PPVa_tr*TPRa_tr)/(PPVa_tr+TPRa_tr))\n",
    "\n",
    "display(Markdown(\"#### Classifier Prediction Performance on Transformed Adult Test Set \"))\n",
    "print(\"Standard accuracy of logistic regression trained on German dataset with LFR mitigation = %f\" % classified_metric_trans_a.accuracy())\n",
    "print(\"Balanced accuracy of logistic regression trained on German dataset with LFRy mitigation = %f\" % bal_acc_a_tr)\n",
    "print(\"F1 score of logistic regression trained on German dataset with LFR mitigation = %f\" % f1_a_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2227b25c4e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Confusion Matrices\n",
    "\n",
    "#Adult\n",
    "cm_a = confusion_matrix(a_transf_test.labels, a_testset_pred_trans.labels)\n",
    "\n",
    "disp_a = ConfusionMatrixDisplay(confusion_matrix=cm_a,\n",
    "                              display_labels=trans_lr_a.classes_)\n",
    "disp_a.plot() \n",
    "\n",
    "#German\n",
    "cm_g = confusion_matrix(g_transf_test.labels, g_testset_pred_trans.labels)\n",
    "\n",
    "disp_g = ConfusionMatrixDisplay(confusion_matrix=cm_g,\n",
    "                              display_labels=trans_lr_g.classes_)\n",
    "disp_g.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-processing algorithm: Adversarial Debiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting transformed dataset and Measuring Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the rest will be finished this weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#This algorithm uses tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adv_deb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d27b2d3b41fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0madv_deb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'adv_deb' is not defined"
     ]
    }
   ],
   "source": [
    "adv_deb.sess_.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-processing algorithm: Calibrated Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison Among Mitigation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Prediction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif360",
   "language": "python",
   "name": "aif360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
