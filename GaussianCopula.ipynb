{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704468a6",
   "metadata": {},
   "source": [
    "This is the first model used to generate synthetic dataset, even before using GAN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b876ab",
   "metadata": {},
   "source": [
    "In probability theory and statistics, a copula is a multivariate cumulative distribution function for which the marginal probability distribution of each variable is uniform on the interval [0, 1]. Copulas are used to describe/model the dependence (inter-correlation) between random variables.[1]\n",
    "https://en.wikipedia.org/wiki/Copula_%28probability_theory%29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c45fed",
   "metadata": {},
   "source": [
    "In mathematical terms, a copula is a distribution over the unit cube [0,1]d which is constructed from a multivariate normal distribution over Rd by using the probability integral transform. Intuitively, a copula is a mathematical function that allows us to describe the joint distribution of multiple random variables by analyzing the dependencies between their marginal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5985d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.demo import load_tabular_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c3e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_tabular_demo('student_placements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29568be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>second_perc</th>\n",
       "      <th>high_perc</th>\n",
       "      <th>high_spec</th>\n",
       "      <th>degree_perc</th>\n",
       "      <th>degree_type</th>\n",
       "      <th>work_experience</th>\n",
       "      <th>experience_years</th>\n",
       "      <th>employability_perc</th>\n",
       "      <th>mba_spec</th>\n",
       "      <th>mba_perc</th>\n",
       "      <th>salary</th>\n",
       "      <th>placed</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17264</td>\n",
       "      <td>M</td>\n",
       "      <td>67.00</td>\n",
       "      <td>91.00</td>\n",
       "      <td>Commerce</td>\n",
       "      <td>58.00</td>\n",
       "      <td>Sci&amp;Tech</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>Mkt&amp;HR</td>\n",
       "      <td>58.80</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-07-23</td>\n",
       "      <td>2020-10-12</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17265</td>\n",
       "      <td>M</td>\n",
       "      <td>79.33</td>\n",
       "      <td>78.33</td>\n",
       "      <td>Science</td>\n",
       "      <td>77.48</td>\n",
       "      <td>Sci&amp;Tech</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>86.5</td>\n",
       "      <td>Mkt&amp;Fin</td>\n",
       "      <td>66.28</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>2020-04-09</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17266</td>\n",
       "      <td>M</td>\n",
       "      <td>65.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>Arts</td>\n",
       "      <td>64.00</td>\n",
       "      <td>Comm&amp;Mgmt</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Mkt&amp;Fin</td>\n",
       "      <td>57.80</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17267</td>\n",
       "      <td>M</td>\n",
       "      <td>56.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>Science</td>\n",
       "      <td>52.00</td>\n",
       "      <td>Sci&amp;Tech</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Mkt&amp;HR</td>\n",
       "      <td>59.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17268</td>\n",
       "      <td>M</td>\n",
       "      <td>85.80</td>\n",
       "      <td>73.60</td>\n",
       "      <td>Commerce</td>\n",
       "      <td>73.30</td>\n",
       "      <td>Comm&amp;Mgmt</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>96.8</td>\n",
       "      <td>Mkt&amp;Fin</td>\n",
       "      <td>55.50</td>\n",
       "      <td>42500.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>2020-09-27</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   student_id gender  second_perc  high_perc high_spec  degree_perc  \\\n",
       "0       17264      M        67.00      91.00  Commerce        58.00   \n",
       "1       17265      M        79.33      78.33   Science        77.48   \n",
       "2       17266      M        65.00      68.00      Arts        64.00   \n",
       "3       17267      M        56.00      52.00   Science        52.00   \n",
       "4       17268      M        85.80      73.60  Commerce        73.30   \n",
       "\n",
       "  degree_type  work_experience  experience_years  employability_perc mba_spec  \\\n",
       "0    Sci&Tech            False                 0                55.0   Mkt&HR   \n",
       "1    Sci&Tech             True                 1                86.5  Mkt&Fin   \n",
       "2   Comm&Mgmt            False                 0                75.0  Mkt&Fin   \n",
       "3    Sci&Tech            False                 0                66.0   Mkt&HR   \n",
       "4   Comm&Mgmt            False                 0                96.8  Mkt&Fin   \n",
       "\n",
       "   mba_perc   salary  placed start_date   end_date duration  \n",
       "0     58.80  27000.0    True 2020-07-23 2020-10-12      3.0  \n",
       "1     66.28  20000.0    True 2020-01-11 2020-04-09      3.0  \n",
       "2     57.80  25000.0    True 2020-01-26 2020-07-13      6.0  \n",
       "3     59.43      NaN   False        NaT        NaT      NaN  \n",
       "4     55.50  42500.0    True 2020-07-04 2020-09-27      3.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fed206",
   "metadata": {},
   "source": [
    "You will notice that there is data with the following characteristics:\n",
    "\n",
    "    There are float, integer, boolean, categorical and datetime values.\n",
    "\n",
    "    There are some variables that have missing data. In particular, all the data related to the placement details is missing in the rows where the student was not placed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1316c",
   "metadata": {},
   "source": [
    "Let us use the GaussianCopula to learn this data and then sample synthetic data about new students to see how well the model captures the characteristics indicated above. In order to do this you will need to:\n",
    "\n",
    "    Import the sdv.tabular.GaussianCopula class and create an instance of it.\n",
    "\n",
    "    Call its fit method passing our table.\n",
    "\n",
    "    Call its sample method indicating the number of synthetic rows that you want to generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daec0bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enas/CopulaGAN/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "/home/enas/CopulaGAN/lib/python3.7/site-packages/scipy/stats/_continuous_distns.py:4965: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/home/enas/CopulaGAN/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:2429: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  Lhat = muhat - Shat*mu\n",
      "/home/enas/CopulaGAN/lib/python3.7/site-packages/scipy/stats/_continuous_distns.py:621: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/home/enas/CopulaGAN/lib/python3.7/site-packages/scipy/optimize/minpack.py:175: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/enas/CopulaGAN/lib/python3.7/site-packages/scipy/optimize/minpack.py:175: RuntimeWarning: The number of calls to function has reached maxfev = 600.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/enas/CopulaGAN/lib/python3.7/site-packages/copulas/univariate/truncated_gaussian.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  a = (self.min - loc) / scale\n",
      "/home/enas/CopulaGAN/lib/python3.7/site-packages/copulas/univariate/truncated_gaussian.py:44: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  b = (self.max - loc) / scale\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.696846008300781e-05 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sdv.tabular import GaussianCopula\n",
    "import time\n",
    "\n",
    "model = GaussianCopula()\n",
    "model.fit(data)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6144383",
   "metadata": {},
   "source": [
    "Notice that the model fitting process took care of transforming the different fields using the appropriate Reversible Data Transforms to ensure that the data has a format that the GaussianMultivariate model can handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ffadc",
   "metadata": {},
   "source": [
    "# Generate synthetic data from the model\n",
    "\n",
    "Once the modeling has finished you are ready to generate new synthetic data by calling the sample method from your model passing the number of rows that we want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472e488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = model.sample(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3da507",
   "metadata": {},
   "source": [
    "This will return a table identical to the one which the model was fitted on, but filled with new data which resembles the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373f6d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>second_perc</th>\n",
       "      <th>high_perc</th>\n",
       "      <th>high_spec</th>\n",
       "      <th>degree_perc</th>\n",
       "      <th>degree_type</th>\n",
       "      <th>work_experience</th>\n",
       "      <th>experience_years</th>\n",
       "      <th>employability_perc</th>\n",
       "      <th>mba_spec</th>\n",
       "      <th>mba_perc</th>\n",
       "      <th>salary</th>\n",
       "      <th>placed</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17374</td>\n",
       "      <td>M</td>\n",
       "      <td>63.53</td>\n",
       "      <td>63.65</td>\n",
       "      <td>Science</td>\n",
       "      <td>64.70</td>\n",
       "      <td>Sci&amp;Tech</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>62.48</td>\n",
       "      <td>Mkt&amp;HR</td>\n",
       "      <td>63.54</td>\n",
       "      <td>27600.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-13</td>\n",
       "      <td>2020-09-19</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17385</td>\n",
       "      <td>F</td>\n",
       "      <td>75.98</td>\n",
       "      <td>62.03</td>\n",
       "      <td>Science</td>\n",
       "      <td>68.11</td>\n",
       "      <td>Comm&amp;Mgmt</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>85.58</td>\n",
       "      <td>Mkt&amp;HR</td>\n",
       "      <td>77.89</td>\n",
       "      <td>31000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17472</td>\n",
       "      <td>F</td>\n",
       "      <td>83.01</td>\n",
       "      <td>69.77</td>\n",
       "      <td>Science</td>\n",
       "      <td>78.12</td>\n",
       "      <td>Sci&amp;Tech</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>85.77</td>\n",
       "      <td>Mkt&amp;Fin</td>\n",
       "      <td>66.46</td>\n",
       "      <td>32500.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>2020-06-15</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17444</td>\n",
       "      <td>M</td>\n",
       "      <td>66.83</td>\n",
       "      <td>66.14</td>\n",
       "      <td>Commerce</td>\n",
       "      <td>58.88</td>\n",
       "      <td>Comm&amp;Mgmt</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>51.69</td>\n",
       "      <td>Mkt&amp;HR</td>\n",
       "      <td>66.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17431</td>\n",
       "      <td>M</td>\n",
       "      <td>59.94</td>\n",
       "      <td>58.36</td>\n",
       "      <td>Science</td>\n",
       "      <td>64.99</td>\n",
       "      <td>Sci&amp;Tech</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>59.20</td>\n",
       "      <td>Mkt&amp;Fin</td>\n",
       "      <td>58.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   student_id gender  second_perc  high_perc high_spec  degree_perc  \\\n",
       "0       17374      M        63.53      63.65   Science        64.70   \n",
       "1       17385      F        75.98      62.03   Science        68.11   \n",
       "2       17472      F        83.01      69.77   Science        78.12   \n",
       "3       17444      M        66.83      66.14  Commerce        58.88   \n",
       "4       17431      M        59.94      58.36   Science        64.99   \n",
       "\n",
       "  degree_type  work_experience  experience_years  employability_perc mba_spec  \\\n",
       "0    Sci&Tech            False                 2               62.48   Mkt&HR   \n",
       "1   Comm&Mgmt            False                 1               85.58   Mkt&HR   \n",
       "2    Sci&Tech            False                 2               85.77  Mkt&Fin   \n",
       "3   Comm&Mgmt            False                 0               51.69   Mkt&HR   \n",
       "4    Sci&Tech            False                 1               59.20  Mkt&Fin   \n",
       "\n",
       "   mba_perc   salary  placed start_date   end_date duration  \n",
       "0     63.54  27600.0    True 2020-02-13 2020-09-19      3.0  \n",
       "1     77.89  31000.0    True 2020-01-14 2020-07-22     12.0  \n",
       "2     66.46  32500.0    True 2020-01-30 2020-06-15      3.0  \n",
       "3     66.87      NaN   False        NaT        NaT      NaN  \n",
       "4     58.33      NaN   False        NaT        NaT      NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852abf4",
   "metadata": {},
   "source": [
    "ou can control the number of rows by specifying the number of samples in the model.sample(<num_rows>). To test, try model.sample(10000). Note that the original table only had ~200 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a220ce",
   "metadata": {},
   "source": [
    "# Save and Load the model\n",
    "\n",
    "In many scenarios it will be convenient to generate synthetic versions of your data directly in systems that do not have access to the original data source. For example, if you may want to generate testing data on the fly inside a testing environment that does not have access to your production database. In these scenarios, fitting the model with real data every time that you need to generate new data is feasible, so you will need to fit a model in your production environment, save the fitted model into a file, send this file to the testing environment and then load it there to be able to sample from it.\n",
    "\n",
    "Let’s see how this process works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660cb6a",
   "metadata": {},
   "source": [
    "# Save and share the model\n",
    "\n",
    "Once you have fitted the model, all you need to do is call its save method passing the name of the file in which you want to save the model. Note that the extension of the filename is not relevant, but we will be using the .pkl extension to highlight that the serialization protocol used is pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bffbcf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9033bb",
   "metadata": {},
   "source": [
    "This will have created a file called my_model1.pkl in the same directory in which you are running SDV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be4909",
   "metadata": {},
   "source": [
    "# Important\n",
    "\n",
    "If you inspect the generated file you will notice that its size is much smaller than the size of the data that you used to generate it. This is because the serialized model contains no information about the original data, other than the parameters it needs to generate synthetic versions of it. This means that you can safely share this my_model.pkl file without the risk of disclosing any of your real data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d02023",
   "metadata": {},
   "source": [
    "# Load the model and generate new data\n",
    "\n",
    "The file you just generated can be sent over to the system where the synthetic data will be generated. Once it is there, you can load it using the GaussianCopula.load method, and then you are ready to sample new data from the loaded instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d72395",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = GaussianCopula.load('my_model1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d44854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = loaded.sample(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58464162",
   "metadata": {},
   "source": [
    "Notice that the system where the model is loaded needs to also have sdv installed, otherwise it will not be able to load the model and use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d525d71",
   "metadata": {},
   "source": [
    "# Specifying the Primary Key of the table\n",
    "\n",
    "One of the first things that you may have noticed when looking at the demo data is that there is a student_id column which acts as the primary key of the table, and which is supposed to have unique values. Indeed, if we look at the number of times that each value appears, we see that all of them appear at most once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcd2668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.student_id.value_counts().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed903e8c",
   "metadata": {},
   "source": [
    "However, if we look at the synthetic data that we generated, we observe that there are some values that appear more than once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823917e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data[new_data.student_id == new_data.student_id.value_counts().index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafadc66",
   "metadata": {},
   "source": [
    "This happens because the model was not notified at any point about the fact that the student_id had to be unique, so when it generates new data it will provoke collisions sooner or later. In order to solve this, we can pass the argument primary_key to our model when we create it, indicating the name of the column that is the index of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ba9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianCopula(\n",
    "   ....:     primary_key='student_id'\n",
    "   ....: )\n",
    "   ....: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fe8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = model.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e26223",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b7d70",
   "metadata": {},
   "source": [
    "As a result, the model will learn that this column must be unique and generate a unique sequence of values for the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.student_id.value_counts().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc846d",
   "metadata": {},
   "source": [
    "# Anonymizing Personally Identifiable Information (PII)\n",
    "\n",
    "There will be many cases where the data will contain Personally Identifiable Information which we cannot disclose. In these cases, we will want our Tabular Models to replace the information within these fields with fake, simulated data that looks similar to the real one but does not contain any of the original values.\n",
    "\n",
    "Let’s load a new dataset that contains a PII field, the student_placements_pii demo, and try to generate synthetic versions of it that do not contain any of the PII fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98472edc",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "The student_placements_pii dataset is a modified version of the student_placements dataset with one new field, address, which contains PII information about the students. Notice that this additional address field has been simulated and does not correspond to data from the real users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pii = load_tabular_demo('student_placements_pii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pii.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianCopula(\n",
    "   ....:     primary_key='student_id',\n",
    "   ....: )\n",
    "   ....: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_pii)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be22f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pii = model.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pii.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59cd0f41",
   "metadata": {},
   "source": [
    "More specifically, we can see how all the addresses that have been generated actually come from the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28320d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pii.address.isin(data_pii.address).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cf1a9",
   "metadata": {},
   "source": [
    "In order to solve this, we can pass an additional argument anonymize_fields to our model when we create the instance. This anonymize_fields argument will need to be a dictionary that contains:\n",
    "\n",
    "    The name of the field that we want to anonymize.\n",
    "\n",
    "    The category of the field that we want to use when we generate fake values for it.\n",
    "\n",
    "The list complete list of possible categories can be seen in the Faker Providers page, and it contains a huge list of concepts such as:\n",
    "\n",
    "    name\n",
    "\n",
    "    address\n",
    "\n",
    "    country\n",
    "\n",
    "    city\n",
    "\n",
    "    ssn\n",
    "\n",
    "    credit_card_number\n",
    "\n",
    "    credit_card_expire\n",
    "\n",
    "    credit_card_security_code\n",
    "\n",
    "    email\n",
    "\n",
    "    telephone\n",
    "\n",
    "    …\n",
    "\n",
    "In this case, since the field is an e-mail address, we will pass a dictionary indicating the category address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ed0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianCopula(\n",
    "   ....:     primary_key='student_id',\n",
    "   ....:     anonymize_fields={\n",
    "   ....:         'address': 'address'\n",
    "   ....:     }\n",
    "   ....: )\n",
    "   ....: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_pii)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8d9bf",
   "metadata": {},
   "source": [
    "As a result, we can see how the real address values have been replaced by other fake addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9dbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pii = model.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pii.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f0c78",
   "metadata": {},
   "source": [
    "Which means that none of the original addresses can be found in the sampled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pii.address.isin(new_data_pii.address).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b46be",
   "metadata": {},
   "source": [
    "# Advanced Usage\n",
    "\n",
    "Now that we have discovered the basics, let’s go over a few more advanced usage examples and see the different arguments that we can pass to our GaussianCopula Model in order to customize it to our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cfc003",
   "metadata": {},
   "source": [
    "# Advanced Usage\n",
    "\n",
    "Now that we have discovered the basics, let’s go over a few more advanced usage examples and see the different arguments that we can pass to our GaussianCopula Model in order to customize it to our needs.\n",
    "How to set transforms to use?\n",
    "\n",
    "One thing that you may have noticed when executing the previous steps is that the fitting process took much longer on the student_placements_pii dataset than it took on the previous version that did not contain the student address. This happens because the address field is interpreted as a categorical variable, which the GaussianCopula one-hot encoded generating 215 new columns that it had to learn afterwards.\n",
    "\n",
    "This transformation, which in this case was very inefficient, happens because the Tabular Models apply Reversible Data Transforms under the hood to transform all the non-numerical variables, which the underlying models cannot handle, into numerical representations which they can properly work with. In the case of the GaussianCopula, the default transformation is a One-Hot encoding, which can work very well with variables that have a small number of different values, but which is very inefficient in cases where there is a large number of values.\n",
    "\n",
    "For this reason, the Tabular Models have an additional argument called field_transformers that let you select which transformer to apply to each column. This field_transformers argument must be passed as a dict which contains the name of the fields for which we want to use a transformer different than the default, and the name of the transformer that we want to use.\n",
    "\n",
    "Possible transformer names are:\n",
    "\n",
    "    integer: Uses a NumericalTransformer of dtype int.\n",
    "\n",
    "    float: Uses a NumericalTransformer of dtype float.\n",
    "\n",
    "    categorical: Uses a CategoricalTransformer without gaussian noise.\n",
    "\n",
    "    categorical_fuzzy: Uses a CategoricalTransformer adding gaussian noise.\n",
    "\n",
    "    one_hot_encoding: Uses a OneHotEncodingTransformer.\n",
    "\n",
    "    label_encoding: Uses a LabelEncodingTransformer.\n",
    "\n",
    "    boolean: Uses a BooleanTransformer.\n",
    "\n",
    "    datetime: Uses a DatetimeTransformer.\n",
    "\n",
    "NOTE: For additional details about each one of the transformers, please visit RDT (https://github.com/sdv-dev/RDT)\n",
    "\n",
    "Let’s now try to improve the previous fitting process by changing the transformer that we use for the address field to something other than the default. As an example, we will use the label_encoding transformer, which instead of generating one column for each possible value, it just replaces each value with a unique integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianCopula(\n",
    "   ....:     primary_key='student_id',\n",
    "   ....:     anonymize_fields={\n",
    "   ....:         'address': 'address'\n",
    "   ....:     },\n",
    "   ....:     field_transformers={\n",
    "   ....:         'address': 'label_encoding'\n",
    "   ....:     }\n",
    "   ....: )\n",
    "   ....: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_pii)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pii = model.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff285152",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pii.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f5382",
   "metadata": {},
   "source": [
    "# Setting Bounds and Specifying Rounding for Numerical Columns\n",
    "\n",
    "By default, the model will learn the upper and lower bounds of the input data, and use that for sampling. This means that all sampled data will be between the maximum and minimum values found in the original dataset for each numeric column. This option can be overwritten using the min_value and max_value model arguments. These values can either be set to a numeric value, set to 'auto' which is the default setting, or set to None which will mean the column is boundless.\n",
    "\n",
    "The model will also learn the number of decimal places to round to by default. This option can be overwritten using the rounding parameter. The value can be an int specifying how many decimal places to round to, 'auto' which is the default setting, or None which means the data will not be rounded.\n",
    "\n",
    "Since we may want to sample values outside of the ranges in the original data, let’s pass the min_value and max_value arguments as None to the model. To keep the number of decimals consistent across columns, we can set rounding to be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianCopula(\n",
    "   ....:     primary_key='student_id',\n",
    "   ....:     min_value=None,\n",
    "   ....:     max_value=None,\n",
    "   ....:     rounding=2\n",
    "   ....: )\n",
    "   ....: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b42cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unbounded_data = model.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unbounded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340d0a8",
   "metadata": {},
   "source": [
    "As you may notice, the sampled data may have values outside the range of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e34e1d",
   "metadata": {},
   "source": [
    "# Exploring the Probability Distributions\n",
    "\n",
    "During the previous steps, every time we fitted the GaussianCopula it performed the following operations:\n",
    "\n",
    "    Learn the format and data types of the passed data\n",
    "\n",
    "    Transform the non-numerical and null data using Reversible Data Transforms to obtain a fully numerical representation of the data from which we can learn the probability distributions.\n",
    "\n",
    "    Learn the probability distribution of each column from the table\n",
    "\n",
    "    Transform the values of each numerical column by converting them to their marginal distribution CDF values and then applying an inverse CDF transformation of a standard normal on them.\n",
    "\n",
    "    Learn the correlations of the newly generated random variables.\n",
    "\n",
    "After this, when we used the model to generate new data for our table using the sample method, it did:\n",
    "\n",
    "    Sample from a Multivariate Standard Normal distribution with the learned correlations.\n",
    "\n",
    "    Revert the sampled values by computing their standard normal CDF and then applying the inverse CDF of their marginal distributions.\n",
    "\n",
    "    Revert the RDT transformations to go back to the original data format.\n",
    "\n",
    "As you can see, during these steps the Marginal Probability Distributions have a very important role, since the GaussianCopula had to learn and reproduce the individual distributions of each column in our table. We can explore the distributions which the GaussianCopula used to model each column using its get_distributions method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5bc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianCopula(\n",
    "   ....:     primary_key='student_id',\n",
    "   ....:     min_value=None,\n",
    "   ....:     max_value=None\n",
    "   ....: )\n",
    "   ....: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad55d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08913e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = model.get_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132149ba",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "In this list we will see multiple distributions for each one of the columns that we have in our data. This is because the RDT transformations used to encode the data numerically often use more than one column to represent each one of the input variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d05375",
   "metadata": {},
   "source": [
    "Let’s explore the individual distribution of one of the columns in our data to better understand how the GaussianCopula processed them and see if we can improve the results by manually specifying a different distribution. For example, let’s explore the experience_years column by looking at the frequency of its values within the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.experience_years.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0675a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.experience_years.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ceca8",
   "metadata": {},
   "source": [
    "By observing the data we can see that the behavior of the values in this column is very similar to a Gamma or even some types of Beta distribution, where the majority of the values are 0 and the frequency decreases as the values increase.\n",
    "\n",
    "Was the GaussianCopula able to capture this distribution on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23551cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions['experience_years']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142a079",
   "metadata": {},
   "source": [
    "It seems that it was not, as it rather thought that the behavior was closer to a Gaussian distribution. And, as a result, we can see how the generated values now contain negative values which are invalid for this column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d47124",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.experience_years.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df01250",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.experience_years.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a06263",
   "metadata": {},
   "source": [
    "Let’s see how we can improve this situation by passing the GaussianCopula the exact distribution that we want it to use for this column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a0a0e",
   "metadata": {},
   "source": [
    "# Setting distributions for individual variables\n",
    "\n",
    "The GaussianCopula class offers the possibility to indicate which distribution to use for each one of the columns in the table, in order to solve situations like the one that we just described. In order to do this, we need to pass a field_distributions argument with dict that indicates the distribution that we want to use for each column.\n",
    "\n",
    "Possible values for the distribution argument are:\n",
    "\n",
    "    univariate: Let copulas select the optimal univariate distribution. This may result in non-parametric models being used.\n",
    "\n",
    "    parametric: Let copulas select the optimal univariate distribution, but restrict the selection to parametric distributions only.\n",
    "\n",
    "    bounded: Let copulas select the optimal univariate distribution, but restrict the selection to bounded distributions only. This may result in non-parametric models being used.\n",
    "\n",
    "    semi_bounded: Let copulas select the optimal univariate distribution, but restrict the selection to semi-bounded distributions only. This may result in non-parametric models being used.\n",
    "\n",
    "    parametric_bounded: Let copulas select the optimal univariate distribution, but restrict the selection to parametric and bounded distributions only.\n",
    "\n",
    "    parametric_semi_bounded: Let copulas select the optimal univariate distribution, but restrict the selection to parametric and semi-bounded distributions only.\n",
    "\n",
    "    gaussian: Use a Gaussian distribution.\n",
    "\n",
    "    gamma: Use a Gamma distribution.\n",
    "\n",
    "    beta: Use a Beta distribution.\n",
    "\n",
    "    student_t: Use a Student T distribution.\n",
    "\n",
    "    gaussian_kde: Use a GaussianKDE distribution. This model is non-parametric, so using this will make get_parameters unusable.\n",
    "\n",
    "    truncated_gaussian: Use a Truncated Gaussian distribution.\n",
    "\n",
    "Let’s see what happens if we make the GaussianCopula use the gamma distribution for our column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import GaussianCopula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd424d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianCopula(\n",
    "   ....:     primary_key='student_id',\n",
    "   ....:     field_distributions={\n",
    "   ....:         'experience_years': 'gamma'\n",
    "   ....:     },\n",
    "   ....:     min_value=None,\n",
    "   ....:     max_value=None\n",
    "   ....: )\n",
    "   ....: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119df47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3b2e6",
   "metadata": {},
   "source": [
    "After this, we can see how the GaussianCopula used the indicated distribution for the experience_years column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a836578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_distributions()['experience_years']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f7997",
   "metadata": {},
   "source": [
    "And, as a result, we can see how the generated data now have a behavior which is closer to the original data and always stays within the valid values range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fad00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = model.sample(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.experience_years.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.experience_years.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb3346",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "Even though there are situations like the one shown above where manually choosing a distribution seems to give better results, in most cases the GaussianCopula will be able to find the optimal distribution on its own, making this manual search of the marginal distributions necessary on very little occasions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e8008",
   "metadata": {},
   "source": [
    "# Conditional Sampling\n",
    "\n",
    "As the name implies, conditional sampling allows us to sample from a conditional distribution using the GaussianCopula model, which means we can generate only values that satisfy certain conditions. These conditional values can be passed to the conditions parameter in the sample method either as a dataframe or a dictionary.\n",
    "\n",
    "In case a dictionary is passed, the model will generate as many rows as requested, all of which will satisfy the specified conditions, such as gender = M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57661dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = {\n",
    "   ....:     'gender': 'M'\n",
    "   ....: }\n",
    "   ....:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aca2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    " model.sample(5, conditions=conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a622e",
   "metadata": {},
   "source": [
    "It’s also possible to condition on multiple columns, such as gender = M, 'experience_years': 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b284b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = {\n",
    "   ....:     'gender': 'M',\n",
    "   ....:     'experience_years': 0\n",
    "   ....: }\n",
    "   ....: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    " model.sample(5, conditions=conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5829e58",
   "metadata": {},
   "source": [
    "The conditions can also be passed as a dataframe. In that case, the model will generate one sample for each row of the dataframe, sorted in the same order. Since the model already knows how many samples to generate, passing it as a parameter is unnecessary. For example, if we want to generate three samples where gender = M and three samples with gender = F, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718af21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659da8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = pd.DataFrame({\n",
    "   ....:     'gender': ['M', 'M', 'M', 'F', 'F', 'F'],\n",
    "   ....: })\n",
    "   ....: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02225e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
